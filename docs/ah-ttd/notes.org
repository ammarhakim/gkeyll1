# -*- org -*-

* Personal notes
** DONE Get first draft of website on server

   Just make something for now and worry about fixing it later.

   I have now created a website on

   http://www.ammar-hakim.org

   This has papers and will have technical notes.

* High-order moment paper

** 19th October 2009: Symbolic calculations needed for HOM paper.

   [NOT REALLY. All that is needed for eigenvalues but not derive
   equations. Just derive by hand.]

   Write a python script/program to automatically generate all the
   needed matrices/equations needed in the high-order moment
   equations. Python could spit out straight LaTeX that is then
   typeset to visualize the equations.

   Other languages can be considered, however, the time learning them
   should be balanced against maintainability, availability of tools
   and power. 

   One option would be to have python spit out the code for Maxima
   that will in turn be used to compute eigenvalues of the equation
   system.

   Another option would to be integrate the code into Sage, so Maxima
   can compute the eigenvalues.

*** More on this ...

   I have now implemented a simple class called Variable that
   represents a tensor::

     P = Variable('P', 2, 3)

   This will create a rank-2 tensor named "P" each index of which goes
   from (0,1,2). How to use this? One needs the following basic
   operations

   - Contraction with another tensor.
   - Derivate operator (with/without contractions)
   - Combination in expressions using operators.

   For example:
   
     u = Variable('u', 1, 3)
     P = Variable('P', 2, 3)
     uP = Contract(u, [0], P, [0])

   This should create a new variable that is a contraction of the
   first index of "u" and first index of "P".

     uPi = generateAllIndices(uP)
     for up in uPi:
       print uPi.show()

   should print "u[0]*P[0,0]+u[1]*P[1,0]+u[2]*P[2,0]" etc.

*** Just use Maxima's tensor package

    Why not just use Maxima? Should explore this option too.

*** Problems with Maxima: Mathematica?

    Maxima has trouble computing eigenvales of sparse matrices. This
    is a problem and maybe one can try Mathematica.

** 19th October 2009: How to determine nature of non-linear dispersive eqns?

   When writing the HOM paper one needs to understand the structure of
   dispersive equations, i.e. equations that have hyperbolic part and
   oscillating source terms. The structure of linearize equations is
   easy to understand using the dispersion relations. However, what to
   do about non-linear equations?
   
   For example, consider the case of hyperbolic equations. One can
   linearize and derive the dispersion relation. This will always be
   of the form w(k) = lambda*k, where lambda is an eigenvalue of the
   system and k is the wave number. This simply tells us that the
   linearized system supports waves moving at the eigenspeeds of the
   system. It tells us nothing about the possibility of shocks,
   contact discontinuities or rarefaction waves. For this we need to
   consult hyperbolic equation theory. What is the similar set of
   mathematical tools to understand hyperbolic-dispersive equations?

* Lucee Notes

** TODO Implement a getLowerDimSlice() function

   This should allow creating a lower-dimensional array from an array
   of a given dimension. The function should take an array of
   dimensions to keep. For example:
#+BEGIN_EXAMPLE
   int lower[3] = {1, 3, 2};
   int upper[3] = {10, 9, 12};
   Lucee::Region<3, int> rgn(lower, upper);
   Lucee::Array<3, double> arr(rgn);

   int selectDims[2] = {0, 2};
   int missingDimsIdx[1] = {3}
   Lucee::Array<2, double> sl2d = 
     arr.getLowerDimSlice<2>(selectDims, missingDimsIdx);
#+END_EXAMPLE

   This should create a 2D array sl2d with lower bounds {1, 2} and
   upper bounds {10, 12} that shares data with the arr array. The
   indexing semantics are: sl2d(i,j) maps to arr(i,3,j).

   Another way to do this could be:
#+BEGIN_EXAMPLE
   int lower[3] = {1, 3, 2};
   int upper[3] = {10, 9, 12};
   Lucee::Region<3, int> rgn(lower, upper);
   Lucee::Array<3, double> arr(rgn);

   int missingDims[1] = {1};
   int missingDimsIdx[1] = {3}
   Lucee::Array<2, double> sl2d = 
     arr.getLowerDimSlice<2>(missingDims, missingDimsIdx);
#+END_EXAMPLE
     The function declaration in the latter case would be:
#+BEGIN_EXAMPLE
     template <RDIM>
     Array<RDIM, T, INDEXER>
     getLowerDimSlice(const unsigned missingDims[NDIM-RDIM],
       const int missingDimsIdx[NDIM-RDIM]);
#+END_EXAMPLE
   The latter seems better.

** TODO <2010-02-24 Wed> Fix doxygen errors.
** DONE Implement sequencers for arrays.
** TODO Need to add Loggers to Lucee

   Rethink the WarpX loggers for this.

** DONE Generalizing indexers

   Indexer classes may need to be generalized. This is to allow the
   indexing of lower dimensional spaces within the space an indexer
   indexes.

   The indexer classes are is becoming very clunky as there are
   different classes for each NDIM. Need to stream-line this.

   <2010-02-22 Mon> Added a generic base class for all linear
   indexers. This includes row and column major indexers and indexers
   that index sub-spaces indexed by these.

** DONE 27th January 2010: Transition to CMake and documentation

   Scons is a crappy build system. So transition to CMake. Also,
   rename the .cc files to .cpp.

   The Sphinx documentation of individual functions is not needed. The
   API documentation should be ripped out and replaced by more user
   examples. Links to doxygen generated documents should be provided
   from the Sphinx documentation.

** TODO 16th November 2009: On HDF5 I/O classes.

   As the binary I/O system for Lucee is HDF5, a cleaner solution to
   wrapping the HDF5 C-API needs to be developed. A possible solution
   is

     Hdf5FileIo io;

     HdfFileNode fn = io.openFile("myfile.h5", "w+");
     Hdf5GroupNode gn = io.openGroup("myGrp");
     // create a child of "myGrp" node
     Hdf5GroupNode childGn = io.createGroup(gn, "myChild");

** 14th November 2009: On storing simulations

   An efficient and systematic method needs to be developed to store
   simulations and generated data. For this (a) input files, (b) run
   script, and (c) plotting scripts need to be put into version
   control. For analysis, enough generated data for plotting should be
   stored. The full data need not be kept or can be stored on an
   external archival drive. Each simulation should be labeled with an
   integer, say starting from 1000. Each simulation should be
   described in an XML file, which serves as a database. One can
   imagine generating HTML files from the XML file. A directory called
   "paper" should be created in which one or more tex files and final
   images and descriptions should be written. Each figure caption
   should have the list of simulation ids that were used to make the
   plot. A make-file should be provided to generated pdf from the tex
   files. The data to make plots should be stored as HDF5 file. The
   data should be annotated with attributes to indicate units of the
   axis and quantities being plotted. Data files should be
   self-contained, i.e., both the array data and complete annotation
   should be in the same file.

   Example directory structure:

   simulation-record.xml
   paper/
     Makefile
     simulation-results.tex
   1000/
     1000.inp
     1000.sh
   1001/
     1001.inp
     1001.sh

   The structure of the XML file should be:

     <Simulation id="1000">
       <Description>
         Sod-shock with wave-propagation method.
       </Description>
     </Simulation>

     <Simulation id="1001">
       <Description>
         Sod-shock with DG second order.
       </Description>
     </Simulation>

** 14th October 2009: On reproducible research

   How would one publish reproducible research? The convectional
   answer, and the most general solution, is to give access to the
   code as well as the input files used to run simulations. Further,
   the scripts to make the published research need to be
   provided. Although general, however, is not always a practical
   solution as (a) the code may be proprietary and under licence
   agreement (b) the code may be very complex to build and run and may
   need significant effort to learn.
   
   A less general solution can be provided by a hierarchy of access to
   the code/data. The later entries in the list indicate less
   portability and greater effort on part of the author as well as the
   reader.

   - Data and script to make the plots must be provided. This should
     mandatory for publication. The reason is that a reader may not be
     always interested in running the code but getting access only to
     the data.
   - Detailed annotated input files must be provided. These input
     files should indicate the sequence of steps (including time-step)
     to run the simulations and the exact parameters used. The input
     files should be structured such that a dedicated reader can
     understand the precise steps required in the simulation.
   - Code to actually execute the simulation should be provided.

   I consider only the first two levels as mandatory for reproducible
   research. The author is under no obligation to provide the source
   code or spend the time to make the code available to the
   reader. For example: an experimentalist should describe the setup
   in detail so it can be reproduced but is not required to actually
   provide access to the experimental setup.

   The data files could be plain text or HDF5 files. The plotting
   scripts should be python, gnuplot or use some tool that is easily
   and freely available.

   The input file annotation should be provided as an XML file. This
   is because XML files are universal and enormous commercial support
   exists to parse XML.

** 22nd September 2009: Functions in KeyVal pair.

   One should be able to store functions in the KeyVal pair
   object. For example

     KeyVal kv;
     InitialCondition func(4); // construct new function taking 4 args
     kv.addFunction("initalCondition", func);
 
     Function& fnc = kv.get("initalCondition");
     val = fnc.eval(t, x, y, z);

   Not clear on how to implement this: (a) should one make a copy of
   the function object? (b) how would this be specified in an input
   file?

*** Solution to the functions problem.

    The idea here is that one can add a Functor object as described in
    "Modern C++ Programming" book. With this one can add functions or
    objects with operator() and that take a std::vector<double> and
    return std::vector<double>.

** 18th September 2009: Documentation notes. AGAIN!!

   The documentation which one should write is user documentation,
   i.e. on how to use the classes. This should all be HTML with the
   option of pdf for printing. So I am now thinking Lucee should be
   documented with Sphinx rather than texinfo. Will I never find a
   solution to this dilemma?

   Problem with Sphinx is that it is not designed for C/C++ typed
   functions.
   
** 11th September 2009: How to initialize Lucee objects?

   Initializing objects is not a trivial task. The intialization
   process should not be split into many stages. For example, calling
   a series of set methods should not be relied upon. The problem is
   that the order in which the sets are called can not be
   controlled. In many situation one needs a particular order for the
   initialization to work correctly.

   Looks like a warpx/facets like approach will be the best. The
   validation of the inputs should be done outside the class. Once the
   init() method is called the object should set itself up completely,
   without the attendent need to verify the inputs.

   The init() method should take a KeyValTree as its parameter. The
   KeyValTree can be constructed in various ways: through XML files
   like in WarpX or through Lua tables. The latter allows the
   possibility for the KeyVal object to hold pointers to functions.

** 9th September 2009: A way to build interactive Lucee

   Have two panes: a top pane for entering blocks of code (maybe Lua,
   maybe Lisp) and a bottom pane for interaction. User enters code in
   the top pane, hits "Evaluate" button and is put on the
   prompt. There, one can examine the objects created, plot data and
   run solvers/simulations.

   Provide buttons to viz results and initial conditions and meshes.

** TODO Complete transpose(), getRow() and getCol() methods in Matrix.
** TODO Complete the solve() method in LcLinAlgebra.
** Notes on matrix class and linear algebra.

    How to implement transpose operators? For example, several LAPACK
    routines work with flags to indicate transpose. One option would
    be to create a transpose class:

    Lucee::Matrix<double> S(2,3);
    Lucee::Matrix<double> ST = transpose(S);

** DONE Test matrix copy ctor and assignment operators.
** DONE Add copy ctor and assignment operators to vector.
** DONE Complete RowMajor indexer and test it.
** DONE Implement the slice() operator.
** TODO Finish implementing RT in homogenous slab.

    Complete the ADO algorithm so the delta-backward paper can be
    finished.

** TODO Implement a 1D ES-PIC code.

    Replicate Birdsal and Langdon book problems.

** TODO Symmetric matrix.

    Implement a SymmetricMatrix class. This should use the same
    indexing mechanism as used in LAPACK.

** 30th July 2009: Documentation notes. Again.

   A good option for producing Lucee documentation is texinfo
   system. It produces both printed as well as on-line documentation
   from a single source. Also, TeX markup is supported for use in
   printed manuals.

   The style one should adopt is to write the documentation at the
   same time as one writes the code. This will ensure that all code is
   documented when it is written and documentation does not become a
   burden, something to be done later on.

** 27th July 2009: Rethinking Lucee

   Lucee should be a physics first code.

   The basic architecture of Lucee needs to be radically different
   from WarpX or FACETS. The problem with them is that the code which
   runs the simulation is too closely tied to the code which
   implements the algorithms. A clear separation is needed between
   these two aspects of the system.

   The low level code should consist of data-structures (for example,
   arrays), grids and solvers. These objects should be stand alone in
   the sense that they should not rely on being initialized or have
   access to a specific parent object. This decoupling of the basic
   object will allow the creation of complex high-level code to
   control simulations. In fact, the high-level code should be written
   in a high-level language like Common Lisp and be fully compatible
   with it.

   For example, the basic grid class could be constructed::

     Lucee::CartGrid grid("grid");
     double lower[2] = {0.0, 0.0};
     double upper[2] = {1.0, 1.0}
     unsigned cells[2] = {20, 20};
     grid.lower = lower;
     grid.upper = upper;
     grid.cells = cells;
     
     grid.init();

   From C, for example one can do::

     LuceeCartGrid *grid = makeLuceeCartGrid("grid");
     grid->lower = lower;
     grid->upper = upper;
     ...
   
   Solver objects can be created::

     Lucee::Array inpArr("inpArr"), outArr("outArr");
     unsigned shape[2] = {20, 20};
     inArr.shape = shape;

     inArr.init();

     Lucee::Solver fluidSlvr("fluid");
     // set up the solver object

     // append input/output variables
     fluidSlvr.setInpVar(0, inpArr);
     fluidSlvr.setOutVar(0, outArr);

     // solve equation
     double t = 0.01;
     fluidSlvr.advance(t);

   This will allow construction of simulations by stringing together
   sequence of solvers. For example, one can run solvers in a loop::

     double tcurr = 0.0, tend = 1.0;
     double dt = 0.1;
     while (tcurr < tend) {
       fluidSlvr.setInpVar(0, inpArr);
       fluidSlvr.setOutVar(0, outArr);
       fluidSlvr.advance(tcurr, dt);

       // copy output to input array
       copySlvr.setInpVar(0, outArr);
       copySlvr.setOutVar(0, inpArr);
       copySlvr.advance(tcurr, dt); // dt is ignored

       tcurr += dt; // advance current time

     }

   Hence, each object needs a series of methods to (a) set various
   values and fetch them. These should be basic types (int, double,
   string and vectors of these) and directly accessible (b) initialize
   after all sets have been called (c) reset the object after calling
   more sets. In general sets called after init() should be
   ignored. How to ensure this?

** December 2008

*** Notes on KeyValTree

    This needs to be rethought. The keys should be unique per-type and
    not for the complete set. Also, removing sets and keys should be
    supported.

*** TODO Documentation questions and testing examples

    How to indicate that a class is a derived class?

    Make sure that all example code compiled. Maybe create an examples
    directory in the docs directory or under unit?

*** Rename files

    Rename all files to be camel-cased. Also, what are good names for
    the I/O and messaging classes? Current names seem very awkward and
    do not reflect what the classes are for.

    Won't do this. There is no need as long as one is consistent
    throught the project. <2008-12-30 Tue>

*** DONE Fix location where config.h is written

    Where to write config.h file? Writing it out to the lib directory
    does not seem correct as it means recompiling the code when
    building parallel or serial even though nothing else has changed.

    Now writing the config.h to the proper build directory.

*** TODO Complete documentation of all classes.

    Both in-code and text documentation needed to be completed.

*** DONE Add more complete tests for loggers and expression parsers.

    May need to get tests more comprehensive. Also, must figure out a
    way of running the tests automatically from a script.

    <2010-02-22 Mon> Using Ctest for automatic testing now.

*** Notes

    First targeted applications for Lucee (a) radiation transport in
    slabs, (b) PIC/FDTD simulations, and (c) branched cable equations.

    Eventually (a) fully implicit MHD solver based on NIMROD
    algorithms, (b) hyperbolic solvers using WAVE/DG.

    Cut-cells or body-fitted grids?

*** Notes

   Lucee will be WarpX successor. A new code was started mainly so
   that I can control its development, rather than worry about a bunch
   of grad students messing it up. The code will be well documented
   and will have all public APIs tested. Valgrind will be run on all
   unit and regression tests to ensure that there are no memory leaks
   or other problems in the code.

*** Simulation bootstrap mechanism

    Lucee will generalize the bootstrap mechanism of WarpX. A base
    class will be provided, which will all major top-level object will
    derive from. A ObjectConstructor class will allow one to specify
    the sequence in which the boostrap occurs. Lucee itself will have
    no idea about grids, arrays or solvers. It will simply construct
    the objects in the sequence specified in the ObjectConstructor
    class.

* FACETS and TxFluids notes
** TODO TxFluids: Implement an "alias" for data-structures

   This should work as follows:

     <DataStruct q>
       kind = distArray1D
       numComponents = 5
     </DataStruct>

     <DataStruct vel>
       kind = distArrayAlias1D
       aliasFor = q
       components = [1, 2, 3]
     </DataStruct>

   This should create a 3 element array 'vel'. Modifying component 0
   of 'vel' should modify component 1 of q.

** TODO TxFluids: Implement a FDTD solver

   Add a new FDTD updater to solve Maxwell equations. Make it work in
   any dimension, later in general geometries. This is to gain
   experience in writing dimension independent manner.

** TODO TxFluids: Write a PointCloud class

   This will be a starting point to get a particle infrastructure in
   TxFluids.

     TxfPointCloud& pc = this->getIn<TxfPointCloud>(0);
     TxfPointCloudItr pcItr = pc.createItr();
     pc.setItr(pcItr, N); // set iterator to Nth point

     // get the coordinates
     double x = pcItr.getX();
     double y = pcItr.getY();
     double x = pcItr.getZ();

     // get the extra variables
     for (unsigned i=0; i<pcItr.getNumComponents(); ++i)
       std::cout << pcItr[i] << std::endl;

     // create a new point and add it to cloud
     TxfPointCloudItr& pcNew = pc.createNewPoint();
     // set its coordinates
     pcNew.setX(0.0);
     pcNew.setY(0.0);
     pcNew.setZ(0.0);

     // set its weight
     pcNew[0] = 1.0;
     
   From the input file one can create the point cloud:

     <DataStruct rays>
       kind = pointCloud
       numComponents = 1 # store weight in addition to position
     </DataStruct>
     
** FACETS: Disable decomposition of grid if a flag is specified

   This is to avoid the decomp of a small grid in a large simulation.

** TODO FACETS: Call dtors for FMCFM handles in the C++ wrapper classes

   Will need to modify the FmTransportModel and children classes for
   this.

** TODO TxFluids: store last inserted data in dynVector

   Presently the last inserted data is not available in the dynVector
   once the vector is flushed out. This should be fixed.

* Style guide

  - All classes and functions should be in namespace Lucee.
  - Use exactly two spaces to indent lines.
  - Pass/return pointers when handing over management of an object. In
    all other cases use references.
  - Make functions const-correct whenever possible. This may mean
    declaring some private members mutable.
  - Comment so that doxygen does not produce any errors. Use terse,
    but grammatically correct English for comments.
  - Put braces on their own lines.
  - Use a space between the keyword "template" and the opening angle
    bracket.
  - Do not use a space between name of a function/method and opening
    parenthesis.

* WarpX Notes

** Restructring WarpX

   What is needed in a good plasma physics solver? There seems no need
   to modify the core infrastructure of warpx but simply clean it up,
   document it thouroughly and make sure that solvers are robust.

   - A robust hyperbolic equation solver. This is the wave propagation
     scheme.

   - A robust Euler solver, divergence free Maxwell solver, MHD solver
     with and without heat transport (Braginskii).

   - A robust Poisson solver.

   All the above should work on both rectangular geometry as well as
   body fitted grids.

** Febuary 2009

*** Integrating Lucee into WarpX

    The core WarpX library needs to be slowly migrated to Lucee
    code. For now Lucee core code will be copied into WarpX and the
    Lucee namepsace will be replaced by WarpX. Then typedefs (or
    defines) will be introduced to make the rest of the code to use Wx
    instead of the WarpX namespace. Maybe just use the full
    namespacing?

    This needs to be done so that the basic framework is well
    documented and tested.

*** More work on general geometry

    For wave2d:

    - Redo the CFL checking code to make sure we use the proper cell
      volume for this.
    - Complete the transverse solvers for use in wave2d.
    - Add a new subsolver to read data from an h5 file. This needs to
      support reading of nodal coordinates for use in the general
      geometry subsolvers.
    - Implement wall BCs for PhMaxwell and Euler equations.
    - Convert the output to Vizschema format. Then we can use Visit to
      plot the results.

    For DG:

    - Derive the equations needed to update the solution. For this we
      need to figure out (a) integration for volumes and surfaces, (b)
      basis functions to use, (c) mass-matrix and its inversion.

** January 2009

*** Regression testing notes

    http://www.warpx.org/wiki/index.php?title=Warpx:Community_Portal#Regression_testing_WarpX
*** WarpX general geometry notes

    We have decided to not introduce major changes in the framework
    but use the existing arrays and subsolvers to handle body fitted
    grids. Andree will take the lead and will work in the branch
    geo_jan_08 branch (already created).

    The first step will be get the WAVE algorithm working on
    body-fitted grids. For this we need to first extend the
    WxHyperbolicEqn class interface so that each equation system
    provides a method to rotate the data back and from a local
    coordinate system. These methods will be called:

    void rotateToLocalFrame
    void rotateToGlobalFrame

    I am not completely sure of the signature but this will emerge when
    we start writing the code. We should also provide two more methods

    void rotateToCartLocalFrame
    void rotateToCartGlobalFrame

    These methods will be used for rotating data for use in the
    rectangular grid code. Of course, one can still use
    rotateToLocalFrame method with proper rotation matrices, but it
    would inefficient to do so when the coordinate system is
    rectangular.

    Andree will copy the wave2d class and modify it as needed. Mainly
    we need to add capacity form differencing to the algorithm. See
    LeVeque's book for details. Also, data will need to be rotated
    before and after rp() method. We do not use fluxes in WAVE so this
    should not be a problem for now. Otherwise I think the changes are
    minor.

    The major work will be in computing the various geometrical
    quantities needed for the algorithm. For now lets focus on 2D WAVE
    as described by Randy. For this we need: area of cell, length of
    left and bottom sides, normals to left and bottom sides. This is 7
    scalars in all. Actually, the way Randy formulates the algorithm
    we need the ratio of these quantities in physical space to
    computational space.

    Towards this end we will assume that the grid in the input file is
    in the computational space::

      <grid>
        Type = WxGridBox
	Lower = [0.0, 0.0]
	Upper = [1.0, 2*PI]
	Cells = [10, 50]
	PeriodicDirs = [1]
      </grid>

    Then we will allocate a 7 component array which will hold the
    geometric information::

      <geo>
        Type = WxVariable
	Kind = parArray

	OnGrid = grid
	NumComponents = 7
	GhostCells = [0, 1]
      </geo>

    A new SubSolver will be created which will populate this array
    with the needed elements::

      <calcGeo>
        Type = WxSubSolver
	Kind = exprWaveCalcGeo2d

	OnGrid = grid
	WriteVars = [geo]

	progn = ["r = xc", "theta = yc"]
	exprs = ["r*cos(theta)", "r*sin(theta)"]
	 
      </calcGeo>

    Here we are assuming that the independent variables in
    computational space will be "xc" and "yc". This SubSolver will
    compute the "geo" array based on the expression provided. In the
    future we can imagine creating another subsolver for the DG scheme
    and Poisson solver.

    The algorithms which need to work on body fitted grids will use
    the "geo" array in their ReadVars to get a hold of the geometrical
    quantities.

    Also, for plotting we need the node coordinates. For this we
    should write another SubSolver which just computes the nodal
    coordinates::

      <nodalCoords>
        Type = WxVariable
	Kind = parArray

	OnGrid = grid
	NumComponents = 2
	GhostCells = [0, 1]
      </nodalCoords>

      <calcNodalCoords>
        Type = WxSubSolver
	Kind = exprCalcNodalCoords

	OnGrid = grid
	WriteVars = [nodalCoords]

	progn = ["r = xc", "theta = yc"]
	exprs = ["r*cos(theta)", "r*sin(theta)"]
	 
      </calcNodalCoords>

    This will store the nodal coordinates into the "nodalCoords"
    array. This subsolver will be called at StartOnly step. Thus we
    will have an array of nodes in the output file at each time-step.

    I will be coming to the UW tomorrow and will go over
    implementation details with Andree. Meanwhile, Andree please check
    out the branch::

    svn co svn+ssh://warpx@psicenter.org/warpx/branches/geo_jan_08

    Lets aim to do the following this week: create the geo array,
    initialize another array on the grid and plot that array. Then you
    can move to the WAVE algorithm.

** November 19th
   
*** TODO Add script to generate XMF files from input files
*** TODO Add subsolver to read a given HDF5 file into memory.

    The input file block for this would be something like:

    <reader>
      Type = WxSubSolver
      Kind = h5SeqFileReader
      
      OnGrid = [grid]
      WriteArrays = [qnew]
      
      baseFileName = 'myFile'
      dataNode = /frc/qnew

    </reader>

** November 6th

*** TODO Crash from missing WxFunction

   Fix crash when we do not find WxFunction in the various exprXXX
   subsolvers.

** November 4th

   See http://buildbot.net/trac for possible continuous integration
   system for use in WarpX.

** October 20th

*** Next steps for WarpX

    WarpX has been used successfully for studing various equations and
    algorithms. The next step is to apply it to real plasma
    devices. Bhuvana has already taken the first step (with help from
    me) in the FRC equilibrium problem. I have also performed FRC
    formation using theta-pinch method and merging on jets to produce
    a plasma liner.

    As I see it we need the following to be able to model more complex
    devices (a) ability to setup geometery (b) ability to specify
    complex boundary conditions. 

    We also need to start using better software engineering
    techniques. This is critical given the size and complexity of the
    code. I will tackle the software engineering first.

    Our aim should be to do research which is reproducible. This means
    (a) anyone can download the code and the input file and get the
    same physics results. Anyone can run scripts to reproduce figures
    in our papers and theses (b) the time to run a simulation should
    be the same on the same preferences.

    We are already using a version control system and an automated
    build. We next need to start testing the code on a daily
    basis. For this we need (a) unit tests (b) regression tests.

    Unit tests are small C++ tests which exercise individual
    classes. We have some already in the src/tests directory. Unit
    tests give confidence that basic functionality is maintained as we
    we modify the core code.

    Regression tests are input files which exercise WarpX as a
    whole. These tests ensure that old features keep working as we add
    more. They also additionally serve as examples on how to use
    various features of the code.

** October 29th

*** Cleanup and software engineering

   WarpX main framework code (i.e. everything not in hyperapp) must be
   thoroughly documented and cleaned up. The API documentation needs to
   be generated nightly using doxygen and put on the wiki. User
   documentation needs to be created using LaTeX. Unit tests need to
   be cleaned up and also run.

   Should we just use txtests? Pros: It works and would be very easy
   to setup and use. Cons: Does not store history of results,
   specially timing results. One option would be to get the tests in
   place now and use txtests till we get something better.

   We must introduce a process. We need to balance the need to get
   results quickly v/s long term maintainablility of the code. For
   this we should work in branches all the time. Only the code we
   think works and is one we want to use should be merged into
   branch. This could be tricky to do (need to see if SVN supports
   this). Thus all experiemental work would still be in the branch but
   the trunk would be "pristine". All code in the trunk must be tested
   either through unit tests or regression tests.

*** Notes on performance analysis

    * Component major should be used. All components should be updated
      at the same time. This is default in WarpX. However, for DG,
      there are a lot of components. The means the cache may not be
      large enough to hold the data for the components. Hence it may
      be advantageous to keep array for each equation seperate.
    * Use cachegrind to get cache performance numbers
    * The poor parallel scaling generally results from sending corner
      values using MPI. In this case a lot of time is spent in
      MPI_Wait. The real question is: how to make custom messaging
      patterns for each algorithm? For example if we use one sided
      forward differences we do not need to get lower edges for the
      sub-domains.

    Tools to use: http://www.cs.virginia.edu/stream/ for memory
    bandwidth analysis. Cachegrind for cache performance. Jumpshot for
    messaging analysis.

    WarpX should be run through the valgrind suite of tools
    regularly. See http://valgrind.org/info/tools.html for full
    list. The problem is that the number of possible subSolvers in
    WarpX is very large and it would be close to impossible to profile
    everything. So one option would be to pick specific cases and
    profile them. One could use the regression tests for this.

** October 8th

*** Notes on software engineering

    We must test WarpX more throughly. For this we need to run unit
    test to check all main classes, run regression tests to check
    physics capabilities and maintain record of run times.

    Is it best to use a available tool? I think so specially if it is
    flexible enough to write custom tests and keeps record of the past
    activity.

    Use CPPUnit for unit testing. Hudson for CI?

    https://hudson.dev.java.net/

** September 30th

*** Documentation notes

    How to document warpx? After a lot of experimenting the best
    option seems to be LaTeX. It has everything one needs to beautiful
    typesetting and also support some form of conversion to HTML.

    Features of the documentation needed (a) index generation (b)
    generation of hyperlinks (c) conversion to HTML with all equations
    properly displayed (d) including source code fragments.

    For making index see:
 
    http://www.image.ufl.edu/help/latex/latex_indexes.shtlm

    For putting source code into LaTeX use Pygments-0.11.1
    package. For this one can run the latex fragment through the
    'pygmentize' command and then insert the output into the LaTeX
    file. Then this file can be run through latex to create the pdf
    file.

    This can all be automated. I.e. tex file -> extract special blocks
    of code -> run through pygmentize -> run through latex.

** September 25th

*** DONE Complete wxplot script.

    This should work in most cases of interest to make simple plots
    from 1D and 2D output. Not clear if this should have an
    interactive mode or not.

*** TODO Modify H5 output to do adhere to vizschema.

    Still need to decide how to handle DG coefficients. WriteOnly
    subsolver?

*** Structure of regression tests

    There are multiple directories one for each major equation system
    or feature.

    In each there will be multiple regression tests. Say one is called
    test.pin. Then there will be the following shell scripts (a)
    test_ser.sh for serial test (b) tests_par.sh for parallel test (c)
    tests_plt_ser.sh to plot serial results and (d) tests_plt_par.sh
    to plot parallel results.

    Each test should only write out 1 frame. The time to run the
    advance will be added to a database.

    Large tests (taking long time) should be run only once every few
    days.

    To run the regression tests scons will be used. Each script will
    be executed using the popen command and the results grep-ed to
    check if there are any errors. A sqlite database will be used to
    store the results. The table structure will be as follows.

    | Name | Platform | Date | WarpX version | Status | Run-time |
    |------+----------+------+---------------+--------+----------|

    This will allow us to track the progress of the tests as a
    function of revision number.
** September 16th

*** TODO Add GSL build instructions to Wiki
*** Notes on WarpX branch ah_sep_2008_1

    This branch was created to:

    - Cleanup the code (formatting and documentation).
    - Create a new registration system in which the objects are simply
      added to the libraries without the headache of two different
      lists of object files needing to be specified.
    - Addition of code to compute coil contribution to static magnetic
      fields.
    - Completion of the radiation transport code.
    - Completion of the FDTD code.

    The rad transport code can be simply copied/converted from the
    fermat2 code.

** September 10th

*** TODO Get fermat2 into warpx.

    Should the fermat2 code just be copied? Or rewritten?
** September 9thg

*** TODO Registration code cleanup

   Cleanup registration system so that the object files which have
   registration code in them do not need to be passed on the command
   line for the link line.

   For this introduce namespaces which reflect the directory. For
   example WX_LIB or WX_HYPERAPPS_EULER etc. In this namespace all the
   registration code should go. Then these header files should be
   included in the WxSimulation ctor and the various functions called.

*** TODO Manual decomposition in input file

   Add code to do manual decomposition. This is very useful when doing
   scaling studies.

*** TODO Config.h not being generated properly

    The config.h file is being generated at the end of the compile and
    not at the begining. Why? Need to fix. Once that is done we can
    simply use the config.h file to configure the various libraries.

** August 28th
*** DONE Fix build instructions on wiki for scons 1.0 and petsc

    Andree should do petsc install instructions.
** August 25th

*** TODO Write Navier Stokes solver

    Write a NS solver using wave for hyperbolic fluxes and MacCormick
    for viscous fluxes.

** August 22nd
*** DONE Complete the 1D DG solver with aux variables

    This needs a routine to pack the auxillary variables into a single
    array before passing them to the reimann and flux functions. Also
    complete the component based limiters. Can we replace these by
    wave based limiters?

    Move the rhs calculation code into a base class so the auxSolver
    can reuse this code.
** August 19th 

*** Potential long term problems with new DG solvers

    Although the new DG method we are working on is very flexible, it
    is also highly error prone as all the burden is now on the input
    file writer. For example, for using component based limiters we
    will need to specify the equations being solved 4 times: 2 in the
    DG rhs calc and 2 in the limiters. If we do 3rd order scheme we
    will need to specify it 6 times. Further, it is really hard to
    understand where and when to apply BCs and limiters, which arrays
    need to be sync()-ed etc.

    This means that our input file now is like an assembly
    language. It is really hard to figure out what exactly is
    happening and debugging input files is becoming hard. How to solve
    this problem is not clear to me. I think what we are doing is
    good, but we need to make it easier to use.

    For now I am going to expand the wxinpparse.py script to also have
    macros. This will at least get rid of the repetitive input file
    blocks. This does not solve the debugging issues, though. I am not
    sure what the solution is in the long run. Maybe having a
    scripting language control this process would be
    possible. However, that would have its own set of issues.

*** DONE Integrate PETSC into WarpX

    Also write an example solver which will be of some use to
    us. Maybe an implicit solver for viscous source terms? Or a
    Poisson solver?

    Before doing this I need to fix the build system to spit out the
    config.h file before any file is built. Then the config.h file can
    be used in configuration rather than command line -D flags.
** August 18th

*** TODO Refactor the comboSolver time-stepper.

    We need to add two new time-steppers in WarpX: fixed dt stepper
    and fuzzy dt stepper.

    The fixed dt stepper will take a fixed time-step specified in the
    input file. The total number of frames and number of steps between
    frames will be specified. If any subsolver fails due to the
    time-step being too large, the system will throw an exception
    printing out the needed time step for stability.

    The fuzzy dt time stepper will take variable time steps but will
    not adjust the time step just before writing out the frame. Hence
    the output may be a bit later than specified in the input
    file. This method will prevent very small time steps which is
    causing some problems in the solution, specially for those
    problems in which the flow is highly unstable.

    Three time-stepping modes need to be added: variableDt, fixedDt,
    floatingDt. The variableDt is what we have now. The fixedDt scheme
    will take a Nout and also the number of steps per frame. The
    floatingDt will be same as variableDt but will not adjust the
    time-step before the frame.

** August 17th

*** TODO Fix the WxSolver initialization code

    The code does not complain when a subsolver name is mis-spelt in
    the WxSubSolverStep input file block. It core dumps instead. This
    needs to be fixed ASAP.

    The SyncVars list also needs to be tested for existence of the
    variable in question. In fact, the whole simulation needs to be
    tested to make sure simple errors are avoided.

    One option would be to take another look at the input file
    validation scheme thought out before.

*** Refactoring for WarpX Blue.

    WarpX Blue will be the interactive, scriptable version of
    WarpX. The subsolvers will not need the read and write variable
    lists. This will need a rethink of how the system initialize
    itself.

    The inpput file should only declare grids, variables and
    subsolvers. The actual composition of the subsolvers and the
    parameters to run them with (in particular: time-step, read/write
    variables) should be controlled from a script.

    For this purpose, [[http://www.lua.org][LUA]] will be used. Some C++ wrapper classes will
    be needed to use allow LUA to call C++ code easily. Although the
    LUA to C interface is easy, it is very tedious to use. Maybe
    something along the lines of PyCXX or Boost.Python can be
    developed for LUA-CXX?

*** DONE Add new keyword in subSolverStep for arrays to sync-ed

    We need to add a new keyword, say SyncVars which indicate which
    arrays should be sync-ed after a set of subsolvers are run. This
    needs to be done ASAP or else the new DG code will not work.

    Still need to test this stuff. <2008-08-18 Mon>
** August 14th

*** DONE Call Bhuvana and go over how to implement the input file based DG solver

    The implementation needs to be done ASAP. Else will be difficult
    to get the auxiliary variables programmed up easily.

    <2008-08-14 Thu> Have now prepared an input file describing the
    new system. Several subsolvers need to be implemented. Input files
    are becoming very complex, but there are significant paybacks in
    terms of flexibility.
** August 13th

*** Refactoring of hyperbolic subsolver

    The hyperbolic subsolver needs refactoring. This needs to happen
    in two ways.

    First, by splitting the time advance of the schemes (specially DG)
    into the input file. Thus, the DG subsolver would only compute the
    RHS of the equation system and not advance the solution in
    time. Then, this RHS solver would be used multiple times in the
    input file to advance the solution. This will allow us to explore
    various time stepping schemes (for example Hancock DG) from the
    input file directly. This will also allow performing more flexible
    updates without having to keep modifying the code every time. For
    example, we could now interleave the computation of implicit
    diffusive source terms directly without having to rewrite the
    subsolvers themselves. This step would also require that the
    limiter application be split out. This could be rather tricky but
    worthwhile in the long run.

    Second, the 1d, 2d and 3d solvers need to be unified. This should
    involve using some other way of indexing the arrays rather than
    (i,j,k), maybe space-filling curves or a fully unstructured
    representation. This will open the way for doing general
    geometries in WarpX. For general geometries one also needs each
    equation system to specify the rotation matrices from global to
    local coordinate system and from local to global coordinate
    system.

*** DONE Make a macro system for use in WarpX

    This should allow substitution of elements in a string
    template. Use the python string.Template class or python string
    substitution features. Macros will allow simpler input file
    creation.

    Done. See

    http://www.warpx.org/wiki/index.php?title=WarpX_Preprocessor

*** Study space-filling curves (SFC) for use in indexing

    This will allow for stepping over general cartesian meshes.
    
*** TODO Get relevant SFC references from Aftosmis paper.

** August 12th

*** DONE Fix problem with time-stepping scheme of comboSolver

    Turns out that the time step is not adjusted to maximum allowable
    by the CFL number. Must fix this.

    This was not a problem with the comboSolver at all. The bug was in
    the WxHyperScheme::schemeStep method. Now fixed. <2008-08-18 Mon>
** August 11th

*** DONE Compare ideal MHD to twofluid for q=1000.

    The results should compare well to each other. They do with
    dispersive waves visible in the twofluid solution.

*** TODO WarpX test system.

    Write special set of builders for scons for running regression
    tests for WarpX. This needs the following things.

    First, the tests need to be configured. For this one needs to
    specify (a) the location of the warpx repository, (b) flags to use
    with scons build of WarpX, (c) the location of the directory
    containing the accepted results.

    Second, the code needs to be downloaded from the repo. Once it is
    downloaded, then we need to cd into the warpx/src directory and
    run scons in it to build the code. The parallel and serial
    versions need to be build if specified.

    Third, the tests need to be run. This means: running the
    preprocessor on the input file, running the input file with the
    executable, and finally, comparing the output with accepted
    results. For parallel executable the code needs to be run with the
    number of processors specified.

    There should be means to run an accepted test and store the
    results in the appropriate place.

*** Problems with auxillary variables.

    The auxillary variable need to be advanced every RK step. Why is
    the current implementation not working?

    Bhuvana has fixed problem. Turns out that the auxillary variables
    needed to be set to 0 before computing the RHS for the auxillary
    equations. <2008-08-12 Tue>
