# -*- org -*-

#+TITLE:     Almost Daily Work Notes
#+AUTHOR:    Ammar Hakim
#+EMAIL:     ahakim@pppl.gov
#+LANGUAGE:  en

* August 10th

  The DG updater seems to be working correctly. My initial input file
  was using a time-step that was too large and so the scheme was
  unstable.

  I need to start documenting all the updaters and other Lua objects
  to make it easier to setup a simulation. Without the SimJ it would
  be presently very hard, even for me, to setup a simulation from
  scratch. This documentation should also have an example top-level
  loop which a user can cut-paste-modify into his/her Lua script.

* August 7th-9th

  Worked on the nodal DG updater. Turns out that the structure of this
  updater is exactly the same as the diffusion updater with local DG
  scheme. Hence, it is best to get this to work first. Then once the
  code is augmented with auxillary variables the diffusion operator
  will be easy to implement.

  [11:15 pm] Completed the code for the nodal DG updater. Need to test
  with advection equation.

* August 6th

  Last week I spent two days helping out at the APS/DPP abstract
  sorter's meeting. Most interesting point from this trip was the talk
  with John Luginsland on possible topics for AFOSR proposals.

  Read a lot of the papers on local DG for diffusive terms. I have a
  basic understanding of this now and see the need for careful
  selection of the interface fluxes to control the effective
  stencil. Need to write this stuff up.

** Moving to exteral libraries

   Once the diffusion terms are implemented I need to move to doing
   some infrastructure work, specially the transition to external
   libraries. First step would be to write a new Field class derived from
   Blitz::Array. Then once this is tested the process of replacing the
   current Field class can be completed.

   The transition to eigen and luabind can happen on a slower scale as
   they at present do not bring in any compelling functionality that
   is needed immediately.

* July 27th
  
  Completed the Landau damping notes. This proved to be much more of a
  chore than I first thought. Even now the comparison with the root
  finder is not complete. I am giving this a break for now to move on
  to other things.

* July 26th

  I am going nuts. Too many sqrts of Te, Ti, mass ratios etc etc to
  keep track up. Haaaaaa.

* July 24th

  Completed script to compute roots of dispersion relation. The key
  was to use the complex argument error function which allows easy and
  accurate calculation of the plasma dispersion function. With this a
  simple (and naive) Newton method allows finding the least damped
  root. However, it appears that the initial guess may not be good
  enough and the root finder often fails to find the root. In this
  case picking a better root seems to fix the problem.

* July 23rd

  Updated note JE15 with results in the weakly-damped case on a large
  domain. Results agree well with analytical solutions.

* July 19th-22nd

  Figured out that the discrepancy in damping rates is that I am using
  a periodic domain that is comparable in size to the Debye lenght. To
  get results that match the roots of the dispersion relation the
  periodic box needs to be much larger than the Debye lenght. Have not
  verified this, but will update the Valsov-Poisson notes with this.

* July 16-18th
  
  Wrote up part of the self-consistent Vlasov-Poisson journal
  note. Showed Matt aspects of the code, overview of DG and energy
  conservation with upwind flux etc etc.

* July 12-13th

  Completed a working input file for Landau damping problem. Solutions
  look correct when compared to published results.

** Transition notes

  The transition to Eigen, Blitz and luabind needs to be done. This
  will not be easy: the Lucee::Matrix, Lucee::Array and direct lua
  interfacing is very deeply buried in the code. However, this is the
  correct thing to do as it allows using features of these libraries
  in Gkeyll.

  Probably the best thing to do is to fork Lucee and work in the
  fork. This will allow the algorithm development work to go on in the
  mainline. Then the two can be merged.

  One option would be to introduce a set of multi-component fields,
  including scalar, vector (3 components) and matrix (3x3). Generic
  fields (with arbitrary components) can also be defined as an array
  with one-larger dimension.

  An issue with this: the code that works with the fixed-component
  array will be different from the one that works with arbitrary
  component array. An option would be to hide this behind calls that
  return the scalar array for a specified component.

* July 6th-11th

  Spent some time building Gkeyll on portal. Eigen was a problem, but
  now fixed. More work on trying to understand on how to use
  luabind. This is a very good library and now I think it is best to
  switch to this as soon as possible.

  Setup a initial Lua program for Landau damping. This needs some more
  work for the quasi-neutral case, but the full Vlasov-Poisson case is
  working. Needs more testing with simpler problems, though.

** A note on (not) commenting code

   The most important thing while programming is to keep as much of
   the code as possible in a single screen. It allows easy
   understanding of what is going on without too much bouncing around
   the file.

   For this I need to stop commenting obvious stuff. There is no
   needed to comment well-written code in which the variable and
   function names make it abovious what is being done. This is
   pervasive in Gkeyll (and all the code I write) and I need to break
   this (bad) habit.

   Remember: "Therefore, since brevity is the soul of wit / And
   tediousness the limbs and outward florishes, / I will be brief."

   While working on documentation or visiting a file, also cleanup the
   tedious comments.

* July 5th

  No work on July 4th on notes. However, did get bilder to build
  luajit and ran a few test cases. Significant speedup is seen from
  the JIT compiler.

  Worked on getting GSL shell to build. The dependency on Anti-Grain
  Geometry library has been taken care off. Only the top-level does
  not build, due to the lack of readline (I think). This is not
  strictly needed, at least not for getting GSL shell into
  Gkeyll. However, it would be good to have the shell build so one can
  play around with it.

* July 3rd

  Working on fixed potential Vlasov notes. Should finish this today if
  possible.

* July 2nd

  More fiddling around with Blitz, luabind and Eigen. All look very
  good for integration into Gkeyll. This integration perhaps can wait
  for now, but needs to be done soon and in a single shot.

** Updates on Gkeyll and Vlasov solvers

   It seems that for the last 3 weeks or more not much has been
   acomplished as far as the algorithm development work is
   concerned. However, a lot of software engineering work had been
   acomplished, including getting the portal build completed and
   determination of the correct set of libraries to use. Further, the
   dependence on facetsall is now removed, and the code can be built
   without any svn accounts from Tech-X.

   My frustration with C++ and the build process is wearing off and I
   am now feeling better about the selected tool chain.  The real
   lesson of all this sould searching ist is that is best to minimize
   the amount of C++ code, while increasing the amount of stuff that
   can be done in Lua. For this, as much functionality as possible
   should be used in pre-existing libraries and Gkeyll itself should
   only focus on the algorithms.

   At this point the self-consistent Vlasov solver code is complete
   and test cases needed to be written. Need to get to this ASAP.

* June 30th

** Bitching about programming languages

   Apparently, it is that time of the year, when my computational soul
   descends into existential hell, wondering if all this C++ mess is
   worth anything at all. Is C++ a good programming language? I do not
   think so, but unfortunately it is a necessary evil. For now. To
   mimimize the evil in Gkeyll one must use the minimum possible C++
   and maximum possible Lua.

   Actually, the question is not one of the correct programming
   language but how to best build a flexible application. The correct
   way to do that is blur (or remove) the distinction between the
   implementation and extension language. I.e. the application should
   have small kernel (written in C++, say) and everything else should
   be built on top of it in a language which also serves as the
   extension language. Highly successful example of this the Emacs
   editor which is mostly written in Lisp which is also the extension
   language. Once the installation of new modules becomes easy (as in
   Emacs, just copy code somewhere and load the top-level file) the
   user becomes a developer, modifying the application to suit his
   needs. The application then need not be loaded with features, in
   fact one should strive for the opposite, but provide enough hooks
   for the user to turn the application into a specialized tool.

** A note on Blitz++ in Gkeyll and Lua wrapping

   Blitz++ seems like a good replacement for the N-dimensional array
   classes in Gkeyll. Eigen can provide the matrix and vector
   functions.

   One way to move forward is to have a blitz::Array as a member of
   the Lucee::Field class. Then, a method should return the array to
   updater (or other classes) when needed.

   Wrapping into Lua should be done in such a way as to minimize the
   amount of C/C++ code, but making the low-level code compact and
   putting a lot of the functionality into Lua. Also, the verbosity of
   the low-level code should be minimized by getting reif of the
   excess syntactic noise.

** Note on initialize() method

   Why is there an initialize() method at all. Why isn't a object
   initialized completely in a constructor? For special objects that
   need multi-step initialization, perhaps initialization is needed,
   but not otherwise.

   This is a hang over from the Facets/Vorpal world in which the input
   file is static and not a true programming language. In fact the
   order of the blocks in F/V does not tell anything about the order
   in which objects are created. In Gkeyll, OTH, the order is explicit
   in the Lua program.

* June 29th

  Luabind seems almost magical. Not only are objects wrapped, but they
  can be created and passed around like ordinary lua variables. Seems
  very cool.

** Some questions for luabind usage

   - How to determine lenght of a table?

   - How to determine if a value is present (other than comparing to
     NIL)

   - Perhaps Lucee::LuaTable can become a wrapper around
     luabind::table?

   - Why is a readInput() method needed? Why not simply have a ctor
     that takes the table as a parameter?

   - What is the correct way to use the luabind::object class?

   - It is not documented very well. Never would have figured out how
     to use the luabind::table<> class.

* June 28th

  Played around with Luabind. This seems to be a very good library and
  will make binding of the C++ objects really simple. Need to think of
  how to migrate from current binding system to luabind, but appears
  to be relatively straighforward for most classes/functions.

* June 25th

  Found the problem with the Vlasov free-streaming input file. The
  issue is still unresolved, however, it is "fixed" for now by using
  0.0 as the initial guess for the solution. Basically, what seems to
  happen is that the solution from the previous time-step was being
  used as a initial guess which was giving completely bogus
  answers. Not sure why, but it does indicate a bug in the
  ContFromDisContUpdater code. This is very strange as nothing special
  is done in that code that would cause this problem. But this code
  must be the problem. What is more bizarre is that this is happening
  with the direct solve, so the initial guess should be completely
  ignored.

* June 24th

**  General philosophical notes

    Once the core algorithm is identified, it should be heavily
    optimized for production code. [Isn't this obvious?]

    The aim should be to reduce the C++ code to the minium possible
    and increase the Lua code via Luabind.

    Is C++ the correct language to use? Are there better alternatives
    out there that are portable to all platforms as well as highly
    efficient? Are there better programming models? I do not believe
    this so-called OO model of programming is a good one, and some
    more exploration is required. Specially: how can multi-core
    processors be exploited transparently?

* June 21st

  Yesterday I finally figured out how to get petsc to find lapack/blas
  on portal.pppl.gov. This information is now coded up in a script,
  checked into the gkeyllall repo.

* June 19th
  
  Spent all day building the code on the cluster. The major issues are
  with lapack: petsc is unable to find lapack and hence the whole
  build chain collapses. After a lot of hacking around, managed to
  build Gkeyll with bilder built lapack, but still have not figured
  out how to make petsc find this information.

* June 18th

  At this point most dependence on TX servers is gone. Did some major
  surgery to the build system to make all of this work. Now, gkeyll
  builds with bilder and is installed in the usual nice wat.

  [NOT REALLY: Turns out the one needs to switch to http URL from http
  URL for numpkgs. Otherwise ice.txcorp.com still asks for a
  password.]

* June 15th

  Turns out that the problem with the projection might not be a
  problem after all. On writing a test case that perturbs a continuous
  function to make a discontinuous function, the projection updater
  nicely recreates a reasonable continuous curve. So what is going on
  then? This is giving me a major headache.

** Initializing fields from HDF5

   I need to write a method that allows reading a field from HDF5
   file. This will allow debugging this horrible problem and also open
   the possiblility of doing restarts.

** Lua/C++ API

   At present not a lot of the C++ methods are available via the Lua
   script. Should this interface me made richer? A richer interface
   means more C++ code, but adds significant flexibility to the
   script.

   The aim should be to increase the amount of Lua code, reducing the
   need to write C++. For example, how can an updater be coded in Lua?
   How can a new datastructure be created?

* June 14th

  Figured out the issue with the reconstruction. The problem is that
  the same order is being used for the continuous reconstruction as
  used in the discontinuous function representation. So, if the
  discontinuous function is piece-wise constant one can not use a
  constant continuous function: the result will be a just flat line.

  So the solution is to go to a higher-order continuous
  reconstruction. So for piece-wise linear representation one should
  use piece-wise quadratic reconstruction.

  For this I need to write a new updater to put a low-order polynomial
  on a higher-order basis function, something that can always be
  done. Then the ContFromDisContUpdater can be used to give the
  high-order continous reconstruction.

  Actually, this looks very much like the reconstruction proposed by
  Huynh in his "Flux Reconstruction ..." paper and so this paper needs
  to be looked at again.

  Using a higher-order reconstruction could be a problem: if the
  Hamiltonian H in the Poisson bracket {H,f} is not represented by a
  sub-set of the basis functions used for f then energy is not
  conserved, as shown by the currently used algorithm. So, for this
  problem we are in a bind.

** A note on basis function

   I need to redo the basis function calculations to make them more
   generic, i.e. arbitrary order, specifiable from the Lua
   program. The use of the Serendipity basis was a big mistake and it
   is better to just use Lobatto basis.

   The problem with Lobatto (or Gaussian) is that there are more basis
   functions than in the Serendipity case. However, this is perhaps
   okay as it simplifies the algorithm as well as increases solution
   accuracy.

   The idea is to create a base class that provides a whole set of
   methods to compute different matrices etc needed in a single
   cell. Then, the classes that implement the basis functions
   themselves should uses thes to get the needed data.

   One issue here is that there is significant amount of shared data,
   perhaps not useful for rectangular grids, but certainly useful for
   body-fitted or unstructured grids. Instead of each updater storing
   it for themselves it would be best if the basis functions could do
   it. [Don't they do it already?]

** Time to re-read Lua book

   Now that I have sufficient experience working with Lua and the
   Lua/C binding API it is perhaps time to look again and see if
   things can be improved.

* June 13th

  Spent time trying to debug the "problem" with the weak-form
  updater. Not sure if there really is a problem with this updater and
  it possible that this is the correct behavior. However, it is clear
  that this is *not* the best continuous reconstructed function from
  the discontinuous function.

** The Eigen matrix/vector pakage and Blitz++

   Explored the Eigen package:

   http://eigen.tuxfamily.org/index.php?title=Main_Page

   This looks like a good package to replace Gkeyll's own matrix and
   vector classes. [June 14: This is actually a phenomenal package].

   Although I think the matrix/vectors from Eigen are a good
   replacement, I am also wondering about replacing the Lucee::Array
   class. Blitz++ seems like a good option, but perhaps will prove
   hard or impossible to compile on supercomputers. For now
   Lucee::Array can stay.

** Boost graph library

   Boost graph library seems like a good option to look into when
   developing unstructured meshes. Also BGL might be useful for
   particles.

** A Frankenstein Monster?

   All this dependence on dozen's of libraries makes a software
   project look like a Frankenstein monster. However, this not a bad
   thing, as it helps focus the application on the algorithms and
   physics, while letting others worry about basic infrastructure.

* June 12th

  Updated the DistFuncMomentCalc1D class to compute first as well as
  second moments. Tested by comparing with exact solution. Looks okay,
  but there is significant errors in the integration which I am not
  sure how to resolve, or even if it is resolvable.

  At this point the note on free-streaming Vlasov solutions (with
  specified potential) can be completed.

  Also, now the updater for the self-consistent problem with phi=n
  needs to be written. DONE: This is a copy of the
  FemPoissonStructUpdater with small modifications. The reason for
  doing this is that the code is almost identical and the nasty
  modifications needed to handle periodic BCs is already handled in
  the FemPoissonStructUpdater class.

* June 11th

  Completed the DistFuncMomentCalc1D class to compute the number
  density. No other moments are computed yet.

  Added updater to integrate field over domain. This is relatively
  straightforward, but this code will not work for product of fields
  as aliasing errors will be otherwise introduced. Need to think of
  how to do this more generally. Of course, this can not be done in
  the most general fasion as the quadrature scheme for arbitrary order
  integration would be too hard to compute. (Really?)

  Added updater to record field at a point.

  Tested all the above with Vlasov free-streaming operator simulation.

** A note on nodal basis for moment calculations

   In the moment calculation code the 2D element stores the moment
   matrix, while the 1D element stores the mass matrix (as it
   should). This is not really the correct way to do it: cross
   basis-function quantities should really be in their own class.

   Actually, the whole basis function class hierarchy will need to be
   revisted when generalizing the code to general geometries,
   specially for unstructured grids. It might be best to just work in
   general geometries for the production code and move the current
   nodal elements to the proto directory. [Isn't all of life a
   prototype for something "better"?]

* June 8th

  Computed matrices needed in moment computation. Put in the
  corresponding code in Lucee SerendipityElement2D class.

** De-Tech-X-ing

   At this point it might be best to get rid of the dependence on TX
   software. For this the numpkgs needs to be put somewhere else
   (googlecode is a good place) and txbase and bilder, CMake need to
   copied into Gkeyll sources.

   This will eliminate the need to have facetsall accounts and also
   the dependence on rapidly changing bilder codebase.

   [June 11th] On looking at this some more, it seems not so
   simple. The whole contents of numpkgs will need to moved to
   googlecode which might not be so simple.

   [June 12th] Scott Kruger tells me that TX is moving bilder and
   related packaged to sourceforge. I will move Gkeyll to use the
   sourceforge version once the move is complete.

* May 24th-29th

  Energy can be conserved with DG even with upwinding by carefully
  taking into account the discontinuity in d(phi)/dx. Greg showed this
  for a piece-wise constant scheme and my proof, I believe, extends
  this to DG.

  Fixed plotting errors in the 3rd order scheme. Now the solution
  looks quite smooth. Next need to project it on an even finer grid.
  
  Wrote Lua code to solve Vlasov equation with specified
  potential. The solutions look good, although I need to spend more
  time in understanding the case in which particle trapping occurs.

* May 23rd

  I want to step back and work on the discretization of just the
  kinetic equation, without the coupling to the field equation. This
  will allow some basic tests of the algorithm with fixed (including
  vanishing) potential giving confidence when coupling to the
  potential solve.

** More notes of proof

   The proof of energy conservation with discontinous potential still
   eludes me. Of course, it is possible that energy is not conserved,
   so the question is: can one design a scheme that does conserve
   energy. The answer is yes: a simple central difference scheme will
   do the job. The question then is: can one extend this to DG/FV
   high-order scheme.

* May 22nd

  Completed the proof that energy is conserved even with upwinding and
  irrespective of how the d(phi)/dx term is treated at
  discontinuities. The proof "looks" correct, but I still need to
  think about this whole thing very carefully.

  *Evening*. Turns out that the proof as written is actually not
  correct, and only works when the potential is constrained to be
  smooth. However, I believe that the proof can be fixed by first
  integrating in X and then doing the V integral, rather than the V
  first and leave-X-till-last approach I have taken now.

  *Night* The proof does not work as any method of taking into account
  the jump in phi in x cancels out on summation in the V-direction. So
  it seems that this scheme only works when the potential is
  constrained to be continuous. This is not a good result as a simple
  (central) finite difference scheme does conserve energy exactly.

* May 21st

  Spent last week working on fixing notes and on understanding the
  simplified "drift-kinetic" equation we wish to solve. Worked out
  energy conservation for a simple central difference scheme.

** Notes on portal.pppl.gov build

  Spent time building the code on Portal. Turns out the problem is now
  with Lapack/Blas being built inconsistently between PetSc and
  Gkeyll. Otherwise things seem to work.

** Gkeyll in parallel

   Need to finish the parallel-ization of Gkeyll. The code hangs at
   present in the sync() method and I need to figure out why. Also,
   I need to simplify the input file in parallel so that only minor
   modifications are needed to run the code in parallel. In
   particular, only calls to the sync() method should be required. At
   present the user needs to define an explicit processor
   decomposition, which is very inconvenient.

** General hyperbolic solver with nodal DG

   Need to write a dimension-independent and geometry-independent
   hyperbolic solver for Gkeyll. This is not hard now that all the
   basic infrastructe is worked out and will be very useful for many
   fluid problems we want to tackle in the future.

* May 15-16th

  Spent most of the day writing up the notes from the incompressible
  Euler solver. With the exact solve the energy and enstrophy
  convergence are as expected.

** DONE Fix small things in notes

   Fix order of the Poisson bracket to make it look like an
   operator. Add sentence about not using periodic BCs for the
   potential in the 1D Poisson bracket tests. Fix figure axis and add
   numbers to the energy and enstrophy plots.

   Add energy and enstrophy figures for the vortex waltz problem.

   The end time on the vortex waltz problem plots is not correct. It
   reads t=8, while it should really be t=100.

* May 14th

  Spend all of last week at IPAM workshop on high-energy density
  physics. Not much work on anything else.

  Implemented qaudratures for the 3rd order basis functions.

* May 6th

  On route to LA. Looked more carefully at the code. Printed out all
  the matrices and they look correct. Now I suspect the problem is in
  the way the contribution from the surface terms is being accumulated
  into the solution.

  [1:56 pm PST]: I have found the problem! It was a simple sign error!
  The thing I forgot was that the integration by parts leads to
  opposite signs for the contribution from the surface and volume
  integrals. Now fixed and basic passive advection test works. Whew!

  Did some tests to check energy conservation. Turns out the energy
  histories still have the same funny behaviour as before I was doing
  the full integration. I wonder if the energy is still not being
  computed correctly? One clue: the double shear problem which was
  previously not working correctly even with upwind fluxes now
  actually works. So it seems the solution is correct (or close to
  being correct) but the energy calculations are messed up. Enstrophy
  conevergence also looks good.

* May 4th

  Completed the C++ code needed for the surface integration
  terms. However, the code blows up. This is probably because the
  interpolated basis functions of the surface are not consistent. Need
  to use the left-surface interpolations for the right cell and
  right-surface interpolations for the left cell. Need to make sure
  this is really the case.

* May 3rd

  Spent most of the previous few days debugging the Poisson bracket
  algorithm. Now also have energy computed with the correct quadrature
  order. Added interface methods to get quadrature data on lower and
  upper surfaces of an element. Updated Maxima scripts to compute the
  needed interpolation matrices.

* April 28th

  Found the bug in the volume integral term. Turns out that the matrix
  needed in the volume quadrature was transposed. Fixed and now the
  volume term seems to work fine. Next, to implement the surface
  integral quadrature.

  Also fixed enstrophy calculator to avoid the aliasing error. Now
  enstrophy converges as it should. Whew! Turns out that the "secular
  profile" of total enstrophy was an artifact of insufficient
  integration accuracy.

* April 27th

  Working on the NodalPoissonBracketUpdater with Gaussian
  integration. Completed the volume integral terms, however, the
  solution looks funky (but does not blow up). Now to debug this.

  The code is becoming quiet confusing due the different matrices
  being computed. Need to write this up and also fix the 1017 notes.

* April 26th

  Added code to compute Gaussian quadrature nodes, weights and
  interpolation matrices. For now only the SerendipityElement2D
  polyOrder=1 is supported. Next to add surface quadrature nodes,
  etc.

  Computed the interpolation matrices using Maxima. Copied them to C++
  code.

* April 25th

  The energy and enstrophy conservation problem is most like an
  aliasing issue. One needs to use Gaussian quadrature to perform the
  volume and surface integrals. The code needs to be extended to allow
  arbitrary specification of integration nodes. For example, one can
  imagine a call like

#+BEGIN_EXAMPLE
  nodalBasis->getInterpolationMatrix(unsigned order,
    Lucee::Matrix<double>& interpMat);
#+END_EXAMPLE

  that would get the interpolation matrix from a basis function
  set. Then, using this one can perform the interpolation and hence
  the quadrature.

* April 24th
  
  Spent more time trying to figure out the energy conservation
  problem. Still no good. Used a single vortex as an example to check
  if a stationary vortex has issues: it does, energy increases by
  about the same amount as it does in the double vortex case.

* April 18-19th

  Spend a couple of days doing a set of comprehensive benchmark
  problems, all with exact solutions, for the Poisson bracket
  updater. The updater seems to work very well.

  This is both good news and bad: good as the updater actually works
  as expected, but bad as I am no closer to the resolution of the
  problems I noted in my Arpil 17th notes.

  I now suspect that there could be an aliasing error or perhaps a
  subtle problem in the manner in which the Poisson solve occurs
  between the RK stages.

* April 17th

  The energy conservation issue is driving me nuts. The solution
  "appears" correct but whatever I do the energy error does not change
  with dt! I added surface "leakage" terms, computed the energy using
  chi*phi method, etc, etc but nothing changes.

  Also, there is no difference on going to rk3 and nothing makes the
  central-flux work. So there is something fundamentally wrong which I
  am unable to figure out. Back to the drawing board.

  At this point, I should focus on benchmarking the Poisson bracket
  updater. Perhaps this energy issue will resolve itself in the course
  of time once I get the double shear problem, for example, to work.

* April 16th

  Almost no work on the weekend.

  The mystery of energy conservation (or lack thereof) continues. I
  have carefully checked all code to make sure it is
  correct. Everything looks good. Now it seems to me that the real
  problem is that the gradient is computed with only first-order
  accuracy. This means that the gradient is not periodic, even though
  the solution is. In fact, the gradient even has opposite signs on
  the opposite boundaries! What this means is there is a term missing
  from the total energy conservation which is basically <phi
  n*grad.phi> integrated over the domain boundary. If grad.phi was
  exactly identical on the boundaries, this term would
  vanish. However, if the term was not the same it would contribute a
  non-zero term to the energy. In the solutions, the difference
  between the gradients on opposite sides is quite large.

  To take this into account I need to add the extra term to the energy
  updater. What a PITA.

* April 13 (Friday 13th)

** Scoping rules in Lua and Gkeyll modules

   Turns out that Lua needs explicit use of the "local" keyword for
   variables to be lexically scoped. This is very different than other
   languages in which variables are local to a scope by default. This
   has lead to some miserable problems in Gkeyll.

   The relative complexity of programming up a new Gkeyll simulations
   leads me to believe that there needs to be a module system. A
   developer would write a module, say for a particular problem, and
   provide a list of (simplified) input values that a user needs to
   specify in order to run the simulation. This means a user need not
   know all the gory details needed to run a simulation, but only
   remember a small set of, well documented, variables.

* April 12

  The following is a very bad way to test if a step failed:
  
#+BEGIN_EXAMPLE
  if (dtSuggested < myDt) then
#+END_EXAMPLE

  The reason is that the inequality can get terribly confused due to
  floating point percision errors. Instead should check the status
  flag. All my simulations have this problem, and so need to do a
  massive search and replace.

  For some reason the RK2 and RK3 results look identical. The
  differences are tiny. Why, I am not sure, but seems like RK3 behaves
  just like RK2.

* April 10-11

  Completed DynVector class and added some unit tests. Everything
  works. Now to use it through Lua, but first I need to add an updater
  to compute something useful.

  Completed an updater EnergyFromStreamFunctionUpdater that computes
  the net energy from the streamfunction. This work, or at least seems
  to. Put in total energy diagnostic into the 64x64 simulation. Turns
  out that with rk2() time-stepping the total energy *increases* by
  0.5%. The increase is not much, however, does indicate the mildly
  unstable nature of rk2() scheme. Need to implement rk3().

  Read Holloway paper. Basic point: using asymmetric Hermite
  polynomials for expanding the velcity dependence is better as it
  allows exact conservation of both momentum and energy, solves the
  plasma oscillation problem exactly and also preserves the shape of
  beams launched with specific velocities. As all non-dissapative
  discrete schemes it suffers from recurrence problem, i.e. phase
  mixing is simulated correctly only for a finite time after which the
  exponential decay turns into a an increase to give back the initial
  conditions.

** TODO Fix DataStruct::write() method to use sub-communicators

   The DynVector can not be written by all processors for obvious
   reasons. Hence, the DataStruct::write() method needs to be modified
   to allow a data-structure to take a sub-communicator so only a
   sub-set of processors do the I/O.

* April 9th

  Did a high-resolution simulation of the two-vortex problem. The
  results look good. The next step is to plot all the DOFs and not
  just the lower-left corner. For this I need to use the bi-linear
  representation to compute the solution on a finer mesh.

  I also need to figure out the problem with the double shear
  problem. Why is it "blowing up"? Is it really because of zero
  velocity at a node?

  Need to add the DynVector concept to Gkeyll.

* April 7th

  Studied the convergence of the 3rd and 4th order 1D Poisson
  solver. The schemes actually converge with 4th and 5th order
  accuracy. Perhaps this is an artifact of trying to measure
  asymptotic accuracy as even with 2 elements the solution looks
  rather good.

  Next need to study the 3rd order 2D Poisson solver. After that the
  periodic BCs solver with 2nd and 3rd order. All of this is very
  tedious work but essential to get confidence in the code.

  Setup a two-vortex problem. The solution looks really good even with
  128x128 grid points. Also setup a double shear problem. The solution
  does not look very good: apparently (I think) when the velocity
  switches sign the DG scheme does not work well. Need to investigate
  more as this is a problem with variable coefficient advection
  problems in general.

* April 6th

  Finally, have periodic BCs working with the FEM Poisson
  solver. Next, need to very carefully test it. Turns out that the
  problem was a very subtle one. The periodicity in FEM means that the
  periodic nodes needs to be identified carefully otherwise all hell
  breaks loose. This was probably the worse week of debugging, both
  the math and the code, in a long time. Now I can sleep.

* April 5th

  Why aren't the far away nodes appearing in the stiffness matrix for
  periodic BCs?

* April 4th

  Need to now implement a generic diagnostics mechanism. The first
  step is to add a new DataStruct called (perhaps) DynVec. This is the
  name I used in Facets and is good enough here. An example to store
  the total energy would be

#+BEGIN_EXAMPLE
  energy = DataStruct.DynVec { numComponents = 1 }
#+END_EXAMPLE

  which would create space to store the total energy in the
  system. The actual computation of the energy would take place in a
  special updater. One can imagine doing similar stuff as done for the
  BCs:

#+BEGIN_EXAMPLE
  energyDiag = Diagnostic.Energy {}
  enstrophyDiag = Diagnostic.Enstrophy {}

  diag = Updater.Diagnostics2D {
    onGrid = grid,
    diagnostics = {energyDiag, enstrophyDiag},
  }
  diag:setIn( {field} )
#+END_EXAMPLE

  Worked more on the periodic BCs issue. I think the basic idea is now
  correct and implemented. However, the solution is still
  incorrect. This could be because I am not taking into account the
  effect of the top-right node on the bottom left node. It also looks
  like Dirichlet BCs are being effectively applied. Tomorrow I need to
  print the code out and pore over it very, very carefully.

* April 3rd

  Perhaps I have now figured out the problem with my Poisson solver
  with periodic BCs. The issue is that although the right (and top)
  edges are set correctly, the periodicity on the left edge is not
  taken into account correctly. This causes the system to be
  ill-posed, I think. To fix the effect of the next to last cells on
  the top and right edges will need to be taken into account when
  constructing the stiffness matrix and the sourcet terms, specially
  for the cells on the left and bottom edges. Not done this yet, but
  need to.

  To get out of this periodic BC debugging madness, I setup and ran a
  simulation with two vortices in a box. The results look fine which
  makes me more confident that the basic Poisson bracket and Poisson
  solver algorithms are working correctly.

* April 2nd

  Spent all day trying to find bug in periodic BCs. No good. I now
  suspect that the formulation of the problem in periodic BCs itself
  might be incorrect. For example: for periodic BCs not only the
  solution but also the slope should match. However, this does not
  seem to be happening in the computed solutions, although the
  solution is periodic. Will spend some more time tomorrow otherwise
  will move to implementing a small stand-alone solver to test things.

* March 30

  After much investigation I have realized that the periodic BC code
  is not correct. It seems to work in some situation which led me to
  believe it was working. However, for the double shear problem the
  solution looks completely bogus and very simple tests now show a
  problem in 2D with just 2 cells in the Y-direction (even though
  there is no variation in Y). Spent time debugging but to no avail.

* March 29th

  False start on getting Poisson solver to work with periodic
  BCs. Half the day was wasted till I realized what was going on.

  Modified Poisson solver to work with periodic BCs. For some crazy
  reason the solution looks as if one is applying Dirichlet BCs and
  not periodic BCs. Not sure what is going on, but more staring at the
  code is needed.

  FOUND THE BUG: The problem was that Dirichlet BCs were being applied
  even when periodic BCs were specified. This is just bad programming
  and wasted another 1/2 day. So day is now over.

  Strangely, the KSP solver has no problem converging to a solution
  even when BCs are periodic. Not sure why, as the matrix should not
  posses an inverse in this case. NOTE: This actually does not work in
  general. So had to pin the lower-left corner value to get
  convergence.

  Setup a double shear problem. This is not working and there seems to
  be some problem with the boundary condition.

* March 28th

  Now polyOrder 2 also works. In getting this to work the code had to
  be rearranged a bit, but now will work with any basis
  functions. This generalization includes a loop over direction which
  seems to add a 10% overhead. For some reason the compiler is unable
  to unroll the loops even though the loop size is explicitly set.

  One lesson here is that even small things can have an impact on the
  performace and that the code performs no where close to its optimal
  levels. This is okay for now but later when real physics problems
  are being tackled it might be important to carefully optimize the
  code.

  Added a flag to the Poisson solver to allow a DG field as an
  input. Now we are really ready for the coupled problem.

  Fixed a very nasty but subtle bug in the Poisson solver that was
  giving weird results when the Poisson solver was called multiple
  times. Turns out that the RHS of the poisson equation was not being
  cleared properly before setting it in a time-dependent problem,
  causing the solution to be different even if the source did not
  change between calls.

* March 27th

  Did more basis tests of the Poisson bracket updater. Converted it to
  be more systematic and eventually be used as a proto-type for a
  dimensionally independent DG solver for other hyperbolic systems.

  Tried to compute the matrix-vector multiplies using BLAS. Makes the
  code 5X *slower*. I suspect this is because BLAS has no advantage
  over simple loops when the matrices and vectors are small. Perhaps
  it would make more sense when the complete updater is
  "vectorized". However, it seems there is a lot of room for
  improvement in performance here.

  Added the methods to support polyOrder = 2. However, the Poisson
  updater still needs more work to make it independent of the number
  of nodes on the faces. Will do this tomorrow, getting very tired
  now.

* March 26th

  Completed the surface integral terms needed in the Poisson bracket
  updater. This involve some more work to the basis function classes,
  making them even chubbier. The interface is becoming very large and
  cumbersome and needs to be looked at again, eventually.

  The Poisson bracket updater is not crashing but also does not seem
  to produce the correct results. Need to debug.

  Found bug in the Poisson bracket updater! It was not actually a bug,
  but I had not implemented upwinding which made the solution show
  oscillations on the trailing edge. Once upwinding was implemented
  the algorithm seems to work fine.

  For now I am testing on a problem with only variations in
  X-direction. Next need to clean up the updater and then do more
  careful tests, including in 2D.

** TODO Write up notes on nodal basis functions

   The interface is sufficiently complicated that an explanation is
   required on how to compute the various things needed in the solvers
   (CG and DG) for a new set of basis. Also, the document should
   explain the CG/DG algorithms in context of the inviscid Euler/H-W
   work we are doing now.

** TODO Put gkeyll docs on ammar-hakim.org/gkeyll

   Put the docs and tech-notes for easy reference. We are close to a
   first-application perhaps in drift-wave turbulence as described by
   the Hasegawa-Watakani equations.

* March 22nd

  Completed all basic loops for Poisson bracket operator. Final step
  is to hook in the surface integral terms. For this a "face mass
  matrix" needs to be computed.

  Wrote an input file with constant prescribed streamfunction with
  evolving vorticity. Will use as a test case to test just the Poisson
  bracket operator.

** DONE Fix crash on using duplicate()-ed fields in out

   Turns out that the code is crashing when using fields created using
   the duplicate() Lua method. Need to investigate and fix.

   PROBLEM: The rgnIdx field in the duplicated field is not
   correct. This is probably the cause of the crash. Will fix in the
   morning. Too tired tonight. NEED TO ADD UNIT TEST FOR DUPLICATE
   METHOD TO ENSURE THIS PROBLEM IS CHECKED FOR.

* March 21st

  Computed all matrices needed in the nodal DG solve. Next to hook
  these into the main loop to compute the various terms.

  Spent some time reading about Hasegawa-Wakatani model. Turns out
  this will need more than just a Poisson solve and a Poisson bracket
  operator: extra terms appear which need to be computed. However,
  they are not hard to do and involve just some more application of
  the differentiation matrices. Derivation in Balescu is very
  enlightening as he uses too many symbols making the derivation very
  un-transparent.

  Compiled code on portal. Petsc fails to build, so no Poisson
  solver. Need to spend time on why this is the case. Perhaps on the
  weekend, after the Poisson bracket operator is complete.

** DONE Test EvalOnNodesUpdater on polyOrder = 1 and 2 in 2D

  Need to test this so we know there are no issues with this. For each
  polyOrder there needs to be two tests: one for a CG field and the
  other for DG field.

* March 20th

  Added new method getGradStiffnessMatrix to NodalFiniteElementIfc
  class. This will support the creation of the final needed matrix in
  the nodal DG scheme. The SerendipityElement2D setup functions are
  now looking horrendous. However, eventually I will need to replace
  all of these with numerically (rather than analytically) computed
  basis functions as the current implementation will not work on
  general quadrilateral geometries. At that point need to copy the
  current implementation into the proto directory and rename it so it
  is available to test the new numerical basis.

* March 19th

  Again, no work on weekend. Or almost none. On Sunday night I played
  around with comparing DG with WAVE for 1D Maxwell equations. The
  point here was that in the IBW problem which I did for David S
  before leaving TX one observes that the RF wave decays very rapidly
  as it propagates into the plasma. This is a big issue, and so I
  investigated a 4th order DG scheme for this. Turns out that the 4th
  order DG is much better: in fact, the decay is eliminated and it
  also runs faster! Unfortunately, this will not impact David's
  project at present, but still sent him my findings. Need to write up
  3 pages with my cold fluid results for him. Need to do this soon as
  the Phase II due date is 4/4.

  Found a bug that has been driving me nuts for a while: the modal 1D
  DG was not working when the time-step was being adapted,
  i.e. rejected and retaken. Turns out that after staring at the C++
  code for a very long time (all day today) I realized that the
  problem was really with the Lua script! Basically, when a step is
  retaken somehow the previous state of the solution is lost. This is
  an elementary mistake which has cost me a lot misery. Lesson: do not
  make elementary mistakes.

* March 16th

  Completed methods needed to have 1D Lobatto elements work with
  polyOrder > 1. This proved to be easy but turned into a debugging
  headache due to a "trivial" bug I introduced.

  The methods for 2D Serendipity elements with polyOrder > 2 are
  considerably tricker to implement. Turns out that the missing nodes
  in the interior make the local -> global mapping really
  hairy. However, I think this is now correct.

  On the other hand the methods copyAllDataFromField and
  copyAllDataToField are turning out to be challenging. The mapping
  from a field to the flat array needed in PetSc is very confusing and
  I need to figure this out before the Serendipity elements can be
  used for polyOrder 2.

  Finally figured on how to do the copyAllDataToField and
  copyAllDataToField methods correctly. The basic idea is to provide a
  new method getGlobalIndices() which given the current cell index
  return the list of owned global indices in that cell and the
  corresponding local node numbers. With this the methods are simple
  to implement.

  The Poisson solver now seems to work for polyOrder = 2 (eyeball
  metric). Not yet fully verified, and that is the next thing to do.

* March 15th

  Wrote EvalOnNodesUpdater that initializes a nodal field from a Lua
  function. Seems to work, although more careful testing is
  needed. 

  One major issue is how to plot these fields? VizScheme/Visit does
  not work for such grids, at least now, although plans are afoot at
  TX to extend Visit and VizSchema to support such meshes. An option
  for now is to write a Python script that converts the H5 output to
  VTK which is then used for plotting. Conversion is not such a big
  deal for 2D fields but can become very painful (slow) in 3D.

  Turns out that the polyOrder 2 Serendipity elements are nasty: it is
  not trivial to figure out the number of global nodes or the local ->
  global mapping. My initial simplistic approach is completely wrong,
  which I discovered on trying to solve the Poisson equation using
  these more complicated elements.

  Formulated the nodal DG scheme. At this point I understand how every
  term works. Next to implement this for the Poisson bracket operator.

** A genuine nodal field type

   The nodal CG/DG schemes need a special nodal field type. This
   method of storing the nodal data in extra components of the Field
   class is not a maintainable model for the long run. So how to do
   this? One option is to introduce a field type that depends on the
   NodalFiniteElementIfc derived classes. Essentially, the first step
   would be to define a grid, then a element basis and then create
   fields which take these element basis as input. For example:

#+BEGIN_EXAMPLE

   -- create grid
   grid = Grid.RectCart2D { ... }

   -- create basis
   lobattoBasis = NodalFiniteElement2D.Serendipity {
     onGrid = grid,
   }

   -- now construct fields
   phi = DataStruct.NodalField {
     onBasis = basis,
     numComponents = 5,
     shareCommonNodes = true, -- true for CG, false for DG
     ...
   }
#+END_EXAMPLE

   The nodal fields object now allocates enough space depending on
   "shareCommonNodes" flag which tells it if overlapping nodes between
   cells are shared or not.

* March 14th

  Added more regression tests. Added an optional message to
  UpdaterStatus class that allows updater to tell Lua what happened,
  specially in case of failure.

* March 13th

  Added more regression tests. Even more are needed as the code is now
  undergoing major changes and additions and it is important not to
  regress on existing functionality.

  Converted the Poisson FEM solver into a dimension independent and
  nodal basis function agnostic updater. Tested by comparing with
  exact solution and also did a convergence study. Wrote up a
  Simulation Journal entry on this. The higher than second order basis
  have not been benchmarked yet. This awaits completion of the
  initialization updater for nodal FEM fields.

  The construction of the Petsc matrix (stiffMatrix) is taking a very
  long time. For example, in 2D 64x64 grid the setup takes 50x more
  than the inversion.

  This is a problem I have dealt with before: the solution is to
  pre-allocate the matrix with as much information as possible (number
  of non-zero entries per row is critical, for example). This makes
  the setup much faster. Also, the solve itself is rather
  inefficient. Of course, the defaults are used as-is and so there is
  a lot of room for improvement.

** TODO Fix parallel test

  Turns out that the sodshock test hangs in parallel. Of course this
  needs to be debugged ASAP. This is probably a run-away send/recv
  which might be very nasty to debug. Grr ....

  Perhaps later tonight as I need to focus on the Poisson bracket
  algorithm for now.

*** Note added on 3/19

    Is this a manifestation of the getSendNeighbors and
    getRecvNeighbors?

* March 12th

  No work on weekend.

  Now have the 2D Poisson solver working. This is basis-function
  agnostic as well as dimension agnostic. So should merge the 1D and
  2D updaters into one and test each of these carefully.

  Also, renamed the top-level executable to gkeyll to reflect that
  this software will be used for solution of GKE.

  Added even more functions to the NodalFiniteElementIfc
  class. Several more will be still needed for the DG scheme.

** DONE Make parallel input files work in serial

   There should not be two different input files for serial and
   parallel. The same file should work with both. Also, ensure that
   the write method works with ghost cell-write in parallel.

   One place this can be taken care off is the StructuredGridBase
   class when the decomposition is created: basically, in serial the
   decomposition should not be sought out at all.

* March 9th

  Made the Poisson solver completely agnostic of the basis functions
  used. Now once I figure out how to apply the BCs in 2D the solver
  can be rewritten to be dimensionally indenpendent as well as work on
  a mapped grid.

  Added BCs to 2D Poisson FEM updater. Does not completly work yet.

  Make all Lucee::LuaTable methods const-correct. I do not remember
  why this was not done in the first place. Perhaps laziness or just
  negligence?

** DONE Extend UpdaterStatus to take a message on why step failed

   This will allow some semblance of debugging, specially with
   linear and non-linear system solves with FEM.

* March 8th

  Compared 1D FEM Poisson solver with exact solution. Found that I had
  a sign off. Fixed and now FEM solution compare well with exact
  solution. Still need to do convergence study, etc.

  Also, I need to implement a method to initialize nodal FE/DG
  fields. The two-node Lobatto elements work fine now as the fields
  allow "nodal" storage. Of course, this will not work when there are
  interior nodes to be initialized.

  Working on a 2D Poisson FEM updater. This seems is very similar to
  the 1D updater and perhaps a dimension-independent updater can be
  written. Best would be if it worked also on a mapped grid.

  Somehow need the basis function IFC class to return the manner in
  which the nodes are laid out. Otherwise I do not see how data can be
  extracted from a field in a transparent manner in the Poisson (or
  other) FE updaters. I.e: the updater should really be agnostic of
  the node layout, the number of nodes, etc.

** TODO Convert Poisson solver and related classes (NodalFiniteElementIfc) for parallel

   The Poisson solver and related classes only work in serial. Need to
   convert these to parallel. Perhaps this is not hard, but will need
   to look into the complete chain of classes, including

** TODO Add more regression tests
   
   Need to add many more regression tests to the system.

* March 7th

  Have a working 1D FEM Poisson solver. Needs testing to make sure the
  solutions are correct. I printed out the stiffness matrix and the
  RHS to ensure that they are correct. So KSP inversion should be
  correct, one would hope.

  Extended the field I/O method to allow writing ghost (or part of
  ghost) cells. This ensures the nodes on the right-most (top-most,
  ...) get written to Hdf5 and allow correct viz of FEM type solution.

  This is actually not the correct way as the interior nodes in the
  ghost cells will also be written out. This is not the desired
  behaviour as only the edge nodes are needed. The correct solution
  here is to actually have a nodal FE field that properly takes into
  account shared nodes between cells. This is a much more complex task
  than I am ready to tackle at present and needs to be done in the
  future. See March 6th note on "Flat field" below.

** TODO Should one add a "finalize" method to UpdaterIfc?

   This will allow "unloading" an updater data from Lua script if
   needed. Not really critical for now, but something to keep in mind.

* March 6th

  Working on 1D Poisson solver using FEM method. Setup basic class,
  brought in Petsc and tested that stiffness matrices are correctly
  built. Found a bug: one can not use the '=' operator for copying
  matrix values as this creates a shallow copy of the RHS. Instead,
  the copy() method needs to be used.

  The 2D Poisson solver should not be that much different, which I
  will work on next. Once that is completed I will switch to DG, which
  should be much simpler. Even though DG needs more complicated
  information (Riemann solves, limiters, ...) it is actually an easier
  scheme to implement.

** Flat field for FEM/DG scheme

   It might be valuable to introduce a "flat-field" data structure
   that stores data essentially in a linear array. This field would be
   indexed with two indices (always): cell-index and
   node-index. Additionally, number of components would be
   specifiable. [This basically is just Field2D.]

   The looping into this field would be achieved by specialized
   iterators, that also would allow neighbor calculations.

* March 2nd

  Implemented Euler numericalFlux method. Did a basic test with
  Sod-shock. The results look "almost" correct, however, there is an
  error: the shocks do not move at the correct speed and the results
  do not compare with either exact solution or with miniwarpx
  results. The problem is most likely in the DG updater for multiple
  equations, which I need to find and fix. 

  [The error could be in the normalization coefficients when applied
  to the case of more than one equation].

  The great thing about the DG method is that only the numerical flux
  method is needed, at least for case in which limiters are not
  applied: the complete decomposition is only needed for the limiters.

* February 21-March 1st
  
  Working on FEM Poisson solver. To get this correct I need to define
  nodal basis functions. This has taken longer than I expected because
  I want to implement this in a way that the basis functions can be
  used in DG also.

  Hooked in PetSc build into luceeall. The parallel build fails but
  the serial code is good enough for now.

  Spent a lot of time working out the various serendipity and
  cartesian product nodal basis functions. Finally have all of these
  figured out, at least for rectangular grids. For now this is okay
  for testing, but eventually will need to be extended to general
  quadrilateral cells. In that case the matrices will need to be
  computed numerically (rather than analytically).

  Implemented 1d Lobatto basis function upto polynomial order 3 (4
  node elements). I did this to get write a 1D Poisson solver to get
  some experience with FEM.

* February 20th

  Formulated FEM scheme for 1D Poisson equations. The key step is the
  one that goes from the local stiffness matrix to the global
  stiffness matrix via the connectivity matrix. For 1D Poisson
  equation the resulting discrete system look like a second-order
  central difference approximation for the spatial operator with an
  averge for the source that weights the current note by 2/6 and
  neighbors by 1/3. Next need to write out the 2D version of this.

* February 18th

  Created a repo to house regression tests. Tried to use txtest but
  was too complicated to use, at least for now. Switched to WarpX
  regression system. Needs more work but works fine for now.

* February 17th

  Turns out that the detector based on Krivodonova et. al. is not so
  good. It is not invariant to addition of a constant to the solution
  for advection equation, for example. Need to think of this more
  carefully.

* February 16th

  Explored a possible detector for discontinuities for use in DG
  scheme. This seems to work okay, but needs some more
  exploration. When applied to the DG scheme itself it does not
  improve the solution a whole lot. The problem is that the detector
  kicks in (I think correctly) even in smooth regions as the slopes
  get modified by the DG update. It seems that a good limiter is also
  needed besides a good detector. Otherwise one may save on compute
  time but improve accuracy.

** DONE Apply limiters to initial conditions

  It also occurs to me that the initial condition needs to be limited
  in the DG Lua code.

* February 14th

  Need to look carefully at both wave and DG schemes. The efficiency
  can be probably improved significantly, at least by a few factors if
  not an order of magnitude.

  The DG limiter is terrible. It completely wipes out the smooth
  exterma. Need to develop something better. The Suresh and Huynh
  paper is really dense and hard to understand. Very unlike his flux
  reconstruction paper which is clear and easy to understand.

* February 13th

  Added new methods to the HyperEquation class to project a vector on
  left-eigenvectors and reconstruct them with
  right-eigenvectors. These two operators are inverses of each
  other. I.e. first projecting on left-eigenvectors and then
  reconstructing on right-eigenvectors should give the original vector
  back. These methods were added for use in limiters for DG scheme.

  The HyperEquation class is becoming very "fat". However, this is
  okay as not all methods are required for all schemes. These two new
  methods will also allow (in combination with the numericalFlux
  method) the implementation of the MUSCL scheme in Lucee.

  Added an updater to limit solution and/or projection from
  DG. Implemented characteristic limiter. Should also implement
  componentwise limiter and then updater Euler equation class with the
  methods needed to make it work with DG.

* February 12th

  I changed the modal DG to return a first order forward Euler update
  and not the "tendencies". I am not sure if this is the correct thing
  to do and perhaps it is a mistake. However, this does allow easy
  application of limiters after the first-order update is complete.

* February 10th

  The algorithm now works! The problem was not with the C++ code but
  with the Lua program. Turns out that the accumulate function is
  actually quite confusing to use as the current contents of the
  fields are not reset before accumulation. Of course, this is the
  correct and intended behavior. Perhaps the solution is to introduce
  a new method called "combine" that clears the current content of the
  field and then does the accumulation. This would be like assigning a
  field with a linear combination of other fields. Spent too much time
  on debugging this.

  Compared with miniwarpx solutions. The timing of miniwarpx v/s
  optimized lucee are comparable. However, I am not sure if miniwarpx
  was built with full optimization. I need to check in the code
  somewhere and build it to do a fair comparison.

  The DG efficiency could be improved by careful rearrangement of the
  loops to make sure the updates happen in cache-correct
  sequence. Anyway this is not too critical at this stage.

** DONE Add a 'combine' method to Field

   This will combine a set of fields into a single one. Essentially it
   a call to clear() followed by an accumulate.

** DONE Put miniwarpx into a bitbuket repo

   This is a good code that allows easy comparison for testing. Should
   check it into bitbuket and make sure it can be built. Perhaps even
   CMake it.

* February 9th

  Working on 1D modal DG. This updater returns the increment in the
  solution. Hence, using its output one can easily do any RK
  time-stepping in the Lua code.

  Completed the code for the 1D modal DG method. The algorithm seems
  to be basically working but the solution is slowly increasing. Need
  to investigate why, perhaps there is an error in the normalization.

** DONE Extend 'accumulate' method

  Need to extend the luaAccumulate method to take in arbitrary number
  of fields and coefficients. For example
#+BEGIN_EXAMPLE
  qNew:accumulate(1.0, q, 0.5, dq)
#+END_EXAMPLE
  will set qNew = qNew + q + 0.5*dq.

* February 7th

  Completed ProjectOnBasisUpdater to compute projection of a Lua
  function on Legendre polynomials. The coefficients are stored in
  row-major order.

** DONE Add initialize() to BasicObj class

   Add this method and call it immediately after readInput() method in
   the ObjRegistry::makeLuaObj method (Line 91). This will eliminate
   the need to explicitly call this method.

   With this change *every* Lua script will need to be changed to
   remove the explicit call to initialize().

** TODO Why are in/out not present in the UpdaterIfc table? Fix if needed.

   There perhaps was some reason for this which I no longer
   recall. But it would make life easier if this was a part of the
   Updater table and did not need an explicit step to do.

* February 6th

  Added an interface class for quadrature weights and
  ordinates. Implemented specific case of Gaussian quadrature.

  Need a way to project a function on basis function for use DG. To do
  this the quadrature object should be created and then used to
  initialize a field whose components represent the coefficients of
  expansion.
#+BEGIN_EXAMPLE
  quad = QuadratureRule.Gaussian { numNodes = 2 }

  -- let q be a field and initFunc a Lua function
  q:project(initFunc, plOrder, quad)
#+END_EXAMPLE

  This will intialize the components of q to the projection of
  initFunc on Legendre polynomials of order plOrder. Perhaps in the
  future projection on different basis could also be suppoeted. Note
  that by using the alias method one can currently set the average (or
  projection on P_0) rather easily. However, this will lead to less
  accurate solutions as the higher order coefficients will not be set.

** Bizarre behavior of luaL_ref method

   Seems like luaL_ref pops the stack and leaves it in a very unstable
   situation. This means that after this method is used it is possible
   that the remaining functional parameters might be totally messed
   up. So, luaL_ref should be done *last*.

   One of the lessons here is that I need to start testing the Lua
   scripts so all Lua callable methods are exercised. I am loosing
   confidence in the code due to lack of regression tests. Time to
   pull in txtests.

** A wasted day: project method will not work

   I am unable to figure out a clean way to make the project method
   work. In fact, I now think that it might be too much of a headache
   to do so as the method is becoming horribly complex.

   It is better to write an updater that does this instead. Will do
   that tomorrow. A big waste. A possible solution is to create an
   updater like the following.

#+BEGIN_EXAMPLE
  initField = Updater.ProjectOnBasis1D {
    onGrid = grid,
    numBasis = 2,
    project = function (x,y,z,t)
                -- do something here
              end,
  }
  initField:initialize()
  initField:setOut( {q} )

  -- run initialization updater
  initField:advance( 0.0 )
#+END_EXAMPLE

* January 31th - February 2nd

  Spent a significant amount of time building Lucee on
  portal.pppl.gov. This needed installation of new modules by the
  system admins as well as small tweeks to the code. Also, as usual,
  Lapack/Blas was an issue. For now I have gotten around it by using
  CLapack on portal.

  A rather nasty problem came up between CLapack and the fortran
  Lapack. This is the difference between a pointer to a single char
  (which is a char *) and a C string which is also char *. Turns out
  the Fortran version accepts both of these but the CLapack version
  only accepts the latter (i.e. NULL delimited string). As luck would
  have had it I was using the former. Switched to the latter to fix
  the problems.

  Spent a lot of time refereshing my memory with continous FEM. Turns
  out that the notation and formalism has been really screwed up by
  mathematicians. Now it is next to impossible to read these papers
  and texts without a thorough understanding of functional analysis.

* January 30th

  Need to extend Field class with multiple nodes. Need to take into
  account the possibility of using continous FEM which requires shared
  nodes between neighboring elements.

  Question: should we have a new data-structure, perhaps derived from
  Field or should Field itself be extended?

  One other option is: do not change Field at all. In fact, field
  should not know about "nodes" as nodes mean existence of a grid in
  which the nodes are located. Instead create a new FieldPtr type (or
  extend the existing one) to allow taking into account the nodes. The
  problem with this approach is that now somehow the FieldPtr needs to
  know about nodes. This could be done at construction time for the
  FieldPtr, for example, or set later on.

  One final option: do nothing. Let the user take care of this in the
  updater or functions that work on FEM type fields. This can be
  easily done by the user, but perhaps is not the best way to do it
  (but involves no work on my part). This is the approach I took in
  WarpX. Actually, this is the correct approach in the current
  framework. Introducing nodes does not make any sense as neither
  field or field-ptr can (or should) know about them.

* January 27th

  Working on MultiRegion class. This is taking longer than I expected,
  a classic symptom of a badly designed abstraction. Currently it is
  quite difficult to create the multi-region object due to the steps
  needed in the constructor. Need to simplify it. For example, one can
  imagine instead

#+BEGIN_EXAMPLE
  MultiRegion<2, int> multiRgn;

  int idx = multiRgn.addRegion( myRgn );

  // add more regions. At this point they are all unconnected

  // add connections (0 -> X, 1 -> Y)
  multiRgn.setRegionLowerConnection(idx, 0,
    MultiRegionConnectivity(targetIdx, targetDir, targetSide));

  // add more connections
#+END_EXAMPLE

  The advantage of this scheme is that unconnected sides do not need
  to be explictly added. The disadvantage is that creation phase might
  be longer and the user needs to keep track of the indices returned
  by the multi-region class. Of course, that could be eliminated by
  allowing the user to specify the index and then checking in the
  setRegionLowerConnection etc methods if such an index exists. In
  this case it would look like

#+BEGIN_EXAMPLE
  MultiRegion<2, int> multiRgn;
  multiRgn.addRegion( myIdx, myRgn );
#+END_EXAMPLE

** TODO Complete MultiRegion class

   Finish the iterator access (or get rid of it) and complete the
   code to allow adding connectivity information.
  
* January 19-24th

  Read first 3 chapters for Frisch.

  Added a new class MultiRegion that stores regions connected to each
  other. To avoid ambiguities in the connections the connectivities
  need to be specified in more detail than I initially thought. This
  is specially true when the block are connected to themselves in
  weird ways (branch-cut grids) or there is a direction switch
  involved at the seams.

  Partially read flux reconstruction paper by Huynh. A really good
  paper. The key difference between Huynh and Dumbser/Balsara approach
  is that the latter reconstruct a higher than K order polynomial
  using more information from the neighboring cells. Huynh only
  reconstructs enough to get K order continuous flux.

* January 18th 2012

  Fixed the sync() code and tested it. Seems to work. Will add more
  unit tests to make sure things are working correctly. Also noticed
  that the Field ctors were not seeting up global and local regions
  correctly. Fixed this. Now parallel simulations will be possible
  with Lucee! [Need to make sync() and decomp region to work with
  periodic BCs].

** TODO Add unit tests for getSendNeighbors() method

   I added the getSendNeighbors() method to compute the regions to
   which we should send data. This is not tested yet, although when
   used in the sync() method it seems to work just fine.

** Ctest for regression testing?

   Seems that ctest could be used for regression testing, at least for
   a simple stuff. Perhaps this should be investigated later but for
   now just use txtest as it has all the logic for finding queue on
   different machines.

** DONE Fix bug when send/recv neighbors are not the same

   Turns out that the case when send/recv neighbors are not the same
   has already bitten. When there are zero ghost cells on one (or
   more) edges of each sub-region the send and recv neighbors are
   different. The current getNeighbors() code only computes RECV
   neighbors (i.e. neighbors from which we expect to get
   something). Another call needs to be added for the SEND
   neighbors. This other call will compute neighbors by extending all
   other regions and intersecting with ourselves.

   I found this bug doing unit testing on the sync() code. Goes to
   show the importance of unit tests.

** Ownership of pointers

   In many classes pointers to externally created objects are
   stored. Should these be stored in boost shared pointers instead?
   What happens if the original pointer goes away. Also, in case of
   shared pointer is a consistent use of these needed?

* January 17th 2012

  Completed code to sync() structured fields. This does not work with
  periodic BCs yet.

  To test the sync() code I have had to add a siginificant amount of
  code in various grids and fields. This now allows creating a
  parallel field from C++ (rather than just Lua) and hence makes it
  easier to test.

  One question is: how can more than one region can be handled by a
  processor? This is a bit tricky as currenly the system implicitly
  assumes MPI will run one region on one processor. This needs to
  change.

* January 16th 2012

  Need to add other decomposition methods to allow arbitrary number of
  regions. Also, perhaps a pure Lua decomposition should also be
  allowed?

  If a field is created with `decompose=false` which processor should
  write the data? Currently all procs do this which can cause
  problems. One option is to not to "fix" this. From Lua one can do
  this by checking the rank and write the array if the rank is the
  correct one.

* January 13th 2012

  Extended the Field::writeToFile method to work in parallel. This was
  trickier than I thought as in some constructors the global region
  was not being set correctly. Fixed all this.

  Minor fixup: renamed globalBox -> globalRegion and localBox ->
  localRegion. This makes the code more consistent.

  Now that my facetsall access is enabled again I should be able to
  setup a regression test repo and see how it can be cron-ed at PPPL.

  Also, to allow unit testing I add methods Lucee.getRank() and
  Lucee.getNumProcs() to the top-level "Lucee" module so this
  information can be queried from Lua.

** DONE Add comprehensive unit test for parallel fields

   There are no unit tests for this stuff yet. However, I wrote a lua
   script to create a CartGrid in parallel and made sure that the
   lower and upper bound on each rank was correct. This brings up a
   more general question: how to incorporate unit tests run from Lua
   using the main Lucee executable into the ctest system?

   The ``DataStruct.Field`` block allows both serial and parallel
   fields. Both need to be tested.
  
   I need to test the parallel Field from a unit test. This can be
   done by creating a field in parallel in which each local region is
   computed from a decomp while the same global region is used. This
   should create a field that behaves like a parallel field.

* January 12th 2012

  More reading up on Krommes 02. Made plans with Greg on how to move
  forward with the project. Will implement couple of schemes from
  Peterson & Hammett paper and then flux-reconstruction DG and Shu-DG
  for 2D incompressible flow problem.

* January 11th 2012

  Spent most of the day working on reviewing basic stuff on
  turbulence, reading Krommes's notes and other references. No work on
  Lucee. Eventually need to understand field-theory approach to
  deriving the GKE.

* January 10th 2012

  Creating a new org file for work done at PPPL. Completed a brief
  LDEVP on the parallel field implementation. Registered the
  decomposition objects so they can now be created from Lua. Next step
  is to hook these up the grid and field classes, implement sync() and
  test. Easier said than done.


  Now StructuredGridBase gets the decomposition object and uses it to
  compute the decomposition. Local and global regions are set
  correctly, at least in serial. Need to add tests for this.

  I am having some problems compiling the code in parallel: a bunch of
  undefined-symbol errors are showing up at link line. This probably
  due to a bad MPI build. I need to reactivate my Facetsall
  permissions and rebuild the complete tool chain. Grr ...

  FIXED parallel build problem. I am not sure if this is the correct
  way to do things. But builds for now. Next need to test the
  structured grid in parallel.

** DONE Create a new repo with regression tests.

   Just use TX's txtest system. It is good enough for our needs and
   will be one less thing to maintain.

** TODO Make neighbor calculations for periodic boundaries.

   A significant unresolved issue: how to deal with periodic domains?
   The neighbor calculation code needs to change for that. Essentially
   on each periodic side of the global region (including corners) we
   need to make copies of the global region. This will then give the
   proper neighbors, including self-intersections. Some ambiguity
   exists in the case in which the only one direction is
   periodic. Question: should the periodic conditions include corners
   in this case? I do not know, yet.
