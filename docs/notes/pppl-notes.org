# -*- org -*-

* March 12th

  No work on weekend.

  Now have the 2D Poisson solver working. This is basis-function
  agnostic as well as dimension agnostic. So should merge the 1D and
  2D updaters into one and test each of these carefully.

  Also, renamed the top-level executable to gkeyll to reflect that
  this software will be used for solution of GKE.

  Added even more functions to the NodalFiniteElementIfc
  class. Several more will be still needed for the DG scheme.

** DONE Make parallel input files work in serial

   There should not be two different input files for serial and
   parallel. The same file should work with both. Also, ensure that
   the write method works with ghost cell-write in parallel.

   One place this can be taken care off is the StructuredGridBase
   class when the decomposition is created: basically, in serial the
   decomposition should not be sought out at all.

* March 9th

  Made the Poisson solver completely agnostic of the basis functions
  used. Now once I figure out how to apply the BCs in 2D the solver
  can be rewritten to be dimensionally indenpendent as well as work on
  a mapped grid.

  Added BCs to 2D Poisson FEM updater. Does not completly work yet.

  Make all Lucee::LuaTable methods const-correct. I do not remember
  why this was not done in the first place. Perhaps laziness or just
  negligence?

** TODO Extend UpdaterStatus to take a message on why step failed

   This will allow some semblance of debugging, specially with
   linear and non-linear system solves with FEM.

* March 8th

  Compared 1D FEM Poisson solver with exact solution. Found that I had
  a sign off. Fixed and now FEM solution compare well with exact
  solution. Still need to do convergence study, etc.

  Also, I need to implement a method to initialize nodal FE/DG
  fields. The two-node Lobatto elements work fine now as the fields
  allow "nodal" storage. Of course, this will not work when there are
  interior nodes to be initialized.

  Working on a 2D Poisson FEM updater. This seems is very similar to
  the 1D updater and perhaps a dimension-independent updater can be
  written. Best would be if it worked also on a mapped grid.

  Somehow need the basis function IFC class to return the manner in
  which the nodes are laid out. Otherwise I do not see how data can be
  extracted from a field in a transparent manner in the Poisson (or
  other) FE updaters. I.e: the updater should really be agnostic of
  the node layout, the number of nodes, etc.

** TODO Convert Poisson solver and related classes (NodalFiniteElementIfc) for parallel

   The Poisson solver and related classes only work in serial. Need to
   convert these to parallel. Perhaps this is not hard, but will need
   to look into the complete chain of classes, including

** TODO Add more regression tests
   
   Need to add many more regression tests to the system.

* March 7th

  Have a working 1D FEM Poisson solver. Needs testing to make sure the
  solutions are correct. I printed out the stiffness matrix and the
  RHS to ensure that they are correct. So KSP inversion should be
  correct, one would hope.

  Extended the field I/O method to allow writing ghost (or part of
  ghost) cells. This ensures the nodes on the right-most (top-most,
  ...) get written to Hdf5 and allow correct viz of FEM type solution.

  This is actually not the correct way as the interior nodes in the
  ghost cells will also be written out. This is not the desired
  behaviour as only the edge nodes are needed. The correct solution
  here is to actually have a nodal FE field that properly takes into
  account shared nodes between cells. This is a much more complex task
  than I am ready to tackle at present and needs to be done in the
  future. See March 6th note on "Flat field" below.

** TODO Should one add a "finalize" method to UpdaterIfc?

   This will allow "unloading" an updater data from Lua script if
   needed. Not really critical for now, but something to keep in mind.

* March 6th

  Working on 1D Poisson solver using FEM method. Setup basic class,
  brought in Petsc and tested that stiffness matrices are correctly
  built. Found a bug: one can not use the '=' operator for copying
  matrix values as this creates a shallow copy of the RHS. Instead,
  the copy() method needs to be used.

  The 2D Poisson solver should not be that much different, which I
  will work on next. Once that is completed I will switch to DG, which
  should be much simpler. Even though DG needs more complicated
  information (Riemann solves, limiters, ...) it is actually an easier
  scheme to implement.

** Flat field for FEM/DG scheme

   It might be valuable to introduce a "flat-field" data structure
   that stores data essentially in a linear array. This field would be
   indexed with two indices (always): cell-index and
   node-index. Additionally, number of components would be
   specifiable. [This basically is just Field2D.]

   The looping into this field would be achieved by specialized
   iterators, that also would allow neighbor calculations.

* March 2nd

  Implemented Euler numericalFlux method. Did a basic test with
  Sod-shock. The results look "almost" correct, however, there is an
  error: the shocks do not move at the correct speed and the results
  do not compare with either exact solution or with miniwarpx
  results. The problem is most likely in the DG updater for multiple
  equations, which I need to find and fix. 

  [The error could be in the normalization coefficients when applied
  to the case of more than one equation].

  The great thing about the DG method is that only the numerical flux
  method is needed, at least for case in which limiters are not
  applied: the complete decomposition is only needed for the limiters.

* February 21-March 1st
  
  Working on FEM Poisson solver. To get this correct I need to define
  nodal basis functions. This has taken longer than I expected because
  I want to implement this in a way that the basis functions can be
  used in DG also.

  Hooked in PetSc build into luceeall. The parallel build fails but
  the serial code is good enough for now.

  Spent a lot of time working out the various serendipity and
  cartesian product nodal basis functions. Finally have all of these
  figured out, at least for rectangular grids. For now this is okay
  for testing, but eventually will need to be extended to general
  quadrilateral cells. In that case the matrices will need to be
  computed numerically (rather than analytically).

  Implemented 1d Lobatto basis function upto polynomial order 3 (4
  node elements). I did this to get write a 1D Poisson solver to get
  some experience with FEM.

* February 20th

  Formulated FEM scheme for 1D Poisson equations. The key step is the
  one that goes from the local stiffness matrix to the global
  stiffness matrix via the connectivity matrix. For 1D Poisson
  equation the resulting discrete system look like a second-order
  central difference approximation for the spatial operator with an
  averge for the source that weights the current note by 2/6 and
  neighbors by 1/3. Next need to write out the 2D version of this.

* February 18th

  Created a repo to house regression tests. Tried to use txtest but
  was too complicated to use, at least for now. Switched to WarpX
  regression system. Needs more work but works fine for now.

* February 17th

  Turns out that the detector based on Krivodonova et. al. is not so
  good. It is not invariant to addition of a constant to the solution
  for advection equation, for example. Need to think of this more
  carefully.

* February 16th

  Explored a possible detector for discontinuities for use in DG
  scheme. This seems to work okay, but needs some more
  exploration. When applied to the DG scheme itself it does not
  improve the solution a whole lot. The problem is that the detector
  kicks in (I think correctly) even in smooth regions as the slopes
  get modified by the DG update. It seems that a good limiter is also
  needed besides a good detector. Otherwise one may save on compute
  time but improve accuracy.

** DONE Apply limiters to initial conditions

  It also occurs to me that the initial condition needs to be limited
  in the DG Lua code.

* February 14th

  Need to look carefully at both wave and DG schemes. The efficiency
  can be probably improved significantly, at least by a few factors if
  not an order of magnitude.

  The DG limiter is terrible. It completely wipes out the smooth
  exterma. Need to develop something better. The Suresh and Huynh
  paper is really dense and hard to understand. Very unlike his flux
  reconstruction paper which is clear and easy to understand.

* February 13th

  Added new methods to the HyperEquation class to project a vector on
  left-eigenvectors and reconstruct them with
  right-eigenvectors. These two operators are inverses of each
  other. I.e. first projecting on left-eigenvectors and then
  reconstructing on right-eigenvectors should give the original vector
  back. These methods were added for use in limiters for DG scheme.

  The HyperEquation class is becoming very "fat". However, this is
  okay as not all methods are required for all schemes. These two new
  methods will also allow (in combination with the numericalFlux
  method) the implementation of the MUSCL scheme in Lucee.

  Added an updater to limit solution and/or projection from
  DG. Implemented characteristic limiter. Should also implement
  componentwise limiter and then updater Euler equation class with the
  methods needed to make it work with DG.

* February 12th

  I changed the modal DG to return a first order forward Euler update
  and not the "tendencies". I am not sure if this is the correct thing
  to do and perhaps it is a mistake. However, this does allow easy
  application of limiters after the first-order update is complete.

* February 10th

  The algorithm now works! The problem was not with the C++ code but
  with the Lua program. Turns out that the accumulate function is
  actually quite confusing to use as the current contents of the
  fields are not reset before accumulation. Of course, this is the
  correct and intended behavior. Perhaps the solution is to introduce
  a new method called "combine" that clears the current content of the
  field and then does the accumulation. This would be like assigning a
  field with a linear combination of other fields. Spent too much time
  on debugging this.

  Compared with miniwarpx solutions. The timing of miniwarpx v/s
  optimized lucee are comparable. However, I am not sure if miniwarpx
  was built with full optimization. I need to check in the code
  somewhere and build it to do a fair comparison.

  The DG efficiency could be improved by careful rearrangement of the
  loops to make sure the updates happen in cache-correct
  sequence. Anyway this is not too critical at this stage.

** DONE Add a 'combine' method to Field

   This will combine a set of fields into a single one. Essentially it
   a call to clear() followed by an accumulate.

** DONE Put miniwarpx into a bitbuket repo

   This is a good code that allows easy comparison for testing. Should
   check it into bitbuket and make sure it can be built. Perhaps even
   CMake it.

* February 9th

  Working on 1D modal DG. This updater returns the increment in the
  solution. Hence, using its output one can easily do any RK
  time-stepping in the Lua code.

  Completed the code for the 1D modal DG method. The algorithm seems
  to be basically working but the solution is slowly increasing. Need
  to investigate why, perhaps there is an error in the normalization.

** DONE Extend 'accumulate' method

  Need to extend the luaAccumulate method to take in arbitrary number
  of fields and coefficients. For example
#+BEGIN_EXAMPLE
  qNew:accumulate(1.0, q, 0.5, dq)
#+END_EXAMPLE
  will set qNew = qNew + q + 0.5*dq.

* February 7th

  Completed ProjectOnBasisUpdater to compute projection of a Lua
  function on Legendre polynomials. The coefficients are stored in
  row-major order.

** TODO Add initialize() to BasicObj class

   Add this method and call it immediately after readInput() method in
   the ObjRegistry::makeLuaObj method (Line 91). This will eliminate
   the need to explicitly call this method.

   With this change *every* Lua script will need to be changed to
   remove the explicit call to initialize().

** TODO Why are in/out not present in the UpdaterIfc table? Fix if needed.

   There perhaps was some reason for this which I no longer
   recall. But it would make life easier if this was a part of the
   Updater table and did not need an explicit step to do.

* February 6th

  Added an interface class for quadrature weights and
  ordinates. Implemented specific case of Gaussian quadrature.

  Need a way to project a function on basis function for use DG. To do
  this the quadrature object should be created and then used to
  initialize a field whose components represent the coefficients of
  expansion.
#+BEGIN_EXAMPLE
  quad = QuadratureRule.Gaussian { numNodes = 2 }

  -- let q be a field and initFunc a Lua function
  q:project(initFunc, plOrder, quad)
#+END_EXAMPLE

  This will intialize the components of q to the projection of
  initFunc on Legendre polynomials of order plOrder. Perhaps in the
  future projection on different basis could also be suppoeted. Note
  that by using the alias method one can currently set the average (or
  projection on P_0) rather easily. However, this will lead to less
  accurate solutions as the higher order coefficients will not be set.

** Bizarre behavior of luaL_ref method

   Seems like luaL_ref pops the stack and leaves it in a very unstable
   situation. This means that after this method is used it is possible
   that the remaining functional parameters might be totally messed
   up. So, luaL_ref should be done *last*.

   One of the lessons here is that I need to start testing the Lua
   scripts so all Lua callable methods are exercised. I am loosing
   confidence in the code due to lack of regression tests. Time to
   pull in txtests.

** A wasted day: project method will not work

   I am unable to figure out a clean way to make the project method
   work. In fact, I now think that it might be too much of a headache
   to do so as the method is becoming horribly complex.

   It is better to write an updater that does this instead. Will do
   that tomorrow. A big waste. A possible solution is to create an
   updater like the following.

#+BEGIN_EXAMPLE
  initField = Updater.ProjectOnBasis1D {
    onGrid = grid,
    numBasis = 2,
    project = function (x,y,z,t)
                -- do something here
              end,
  }
  initField:initialize()
  initField:setOut( {q} )

  -- run initialization updater
  initField:advance( 0.0 )
#+END_EXAMPLE

* January 31th - February 2nd

  Spent a significant amount of time building Lucee on
  portal.pppl.gov. This needed installation of new modules by the
  system admins as well as small tweeks to the code. Also, as usual,
  Lapack/Blas was an issue. For now I have gotten around it by using
  CLapack on portal.

  A rather nasty problem came up between CLapack and the fortran
  Lapack. This is the difference between a pointer to a single char
  (which is a char *) and a C string which is also char *. Turns out
  the Fortran version accepts both of these but the CLapack version
  only accepts the latter (i.e. NULL delimited string). As luck would
  have had it I was using the former. Switched to the latter to fix
  the problems.

  Spent a lot of time refereshing my memory with continous FEM. Turns
  out that the notation and formalism has been really screwed up by
  mathematicians. Now it is next to impossible to read these papers
  and texts without a thorough understanding of functional analysis.

* January 30th

  Need to extend Field class with multiple nodes. Need to take into
  account the possibility of using continous FEM which requires shared
  nodes between neighboring elements.

  Question: should we have a new data-structure, perhaps derived from
  Field or should Field itself be extended?

  One other option is: do not change Field at all. In fact, field
  should not know about "nodes" as nodes mean existence of a grid in
  which the nodes are located. Instead create a new FieldPtr type (or
  extend the existing one) to allow taking into account the nodes. The
  problem with this approach is that now somehow the FieldPtr needs to
  know about nodes. This could be done at construction time for the
  FieldPtr, for example, or set later on.

  One final option: do nothing. Let the user take care of this in the
  updater or functions that work on FEM type fields. This can be
  easily done by the user, but perhaps is not the best way to do it
  (but involves no work on my part). This is the approach I took in
  WarpX. Actually, this is the correct approach in the current
  framework. Introducing nodes does not make any sense as neither
  field or field-ptr can (or should) know about them.

* January 27th

  Working on MultiRegion class. This is taking longer than I expected,
  a classic symptom of a badly designed abstraction. Currently it is
  quite difficult to create the multi-region object due to the steps
  needed in the constructor. Need to simplify it. For example, one can
  imagine instead

#+BEGIN_EXAMPLE
  MultiRegion<2, int> multiRgn;

  int idx = multiRgn.addRegion( myRgn );

  // add more regions. At this point they are all unconnected

  // add connections (0 -> X, 1 -> Y)
  multiRgn.setRegionLowerConnection(idx, 0,
    MultiRegionConnectivity(targetIdx, targetDir, targetSide));

  // add more connections
#+END_EXAMPLE

  The advantage of this scheme is that unconnected sides do not need
  to be explictly added. The disadvantage is that creation phase might
  be longer and the user needs to keep track of the indices returned
  by the multi-region class. Of course, that could be eliminated by
  allowing the user to specify the index and then checking in the
  setRegionLowerConnection etc methods if such an index exists. In
  this case it would look like

#+BEGIN_EXAMPLE
  MultiRegion<2, int> multiRgn;
  multiRgn.addRegion( myIdx, myRgn );
#+END_EXAMPLE

** TODO Complete MultiRegion class

   Finish the iterator access (or get rid of it) and complete the
   code to allow adding connectivity information.
  
* January 19-24th

  Read first 3 chapters for Frisch.

  Added a new class MultiRegion that stores regions connected to each
  other. To avoid ambiguities in the connections the connectivities
  need to be specified in more detail than I initially thought. This
  is specially true when the block are connected to themselves in
  weird ways (branch-cut grids) or there is a direction switch
  involved at the seams.

  Partially read flux reconstruction paper by Huynh. A really good
  paper. The key difference between Huynh and Dumbser/Balsara approach
  is that the latter reconstruct a higher than K order polynomial
  using more information from the neighboring cells. Huynh only
  reconstructs enough to get K order continuous flux.

* January 18th 2012

  Fixed the sync() code and tested it. Seems to work. Will add more
  unit tests to make sure things are working correctly. Also noticed
  that the Field ctors were not seeting up global and local regions
  correctly. Fixed this. Now parallel simulations will be possible
  with Lucee! [Need to make sync() and decomp region to work with
  periodic BCs].

** TODO Add unit tests for getSendNeighbors() method

   I added the getSendNeighbors() method to compute the regions to
   which we should send data. This is not tested yet, although when
   used in the sync() method it seems to work just fine.

** Ctest for regression testing?

   Seems that ctest could be used for regression testing, at least for
   a simple stuff. Perhaps this should be investigated later but for
   now just use txtest as it has all the logic for finding queue on
   different machines.

** DONE Fix bug when send/recv neighbors are not the same

   Turns out that the case when send/recv neighbors are not the same
   has already bitten. When there are zero ghost cells on one (or
   more) edges of each sub-region the send and recv neighbors are
   different. The current getNeighbors() code only computes RECV
   neighbors (i.e. neighbors from which we expect to get
   something). Another call needs to be added for the SEND
   neighbors. This other call will compute neighbors by extending all
   other regions and intersecting with ourselves.

   I found this bug doing unit testing on the sync() code. Goes to
   show the importance of unit tests.

** Ownership of pointers

   In many classes pointers to externally created objects are
   stored. Should these be stored in boost shared pointers instead?
   What happens if the original pointer goes away. Also, in case of
   shared pointer is a consistent use of these needed?

* January 17th 2012

  Completed code to sync() structured fields. This does not work with
  periodic BCs yet.

  To test the sync() code I have had to add a siginificant amount of
  code in various grids and fields. This now allows creating a
  parallel field from C++ (rather than just Lua) and hence makes it
  easier to test.

  One question is: how can more than one region can be handled by a
  processor? This is a bit tricky as currenly the system implicitly
  assumes MPI will run one region on one processor. This needs to
  change.

* January 16th 2012

  Need to add other decomposition methods to allow arbitrary number of
  regions. Also, perhaps a pure Lua decomposition should also be
  allowed?

  If a field is created with `decompose=false` which processor should
  write the data? Currently all procs do this which can cause
  problems. One option is to not to "fix" this. From Lua one can do
  this by checking the rank and write the array if the rank is the
  correct one.

* January 13th 2012

  Extended the Field::writeToFile method to work in parallel. This was
  trickier than I thought as in some constructors the global region
  was not being set correctly. Fixed all this.

  Minor fixup: renamed globalBox -> globalRegion and localBox ->
  localRegion. This makes the code more consistent.

  Now that my facetsall access is enabled again I should be able to
  setup a regression test repo and see how it can be cron-ed at PPPL.

  Also, to allow unit testing I add methods Lucee.getRank() and
  Lucee.getNumProcs() to the top-level "Lucee" module so this
  information can be queried from Lua.

** DONE Add comprehensive unit test for parallel fields

   There are no unit tests for this stuff yet. However, I wrote a lua
   script to create a CartGrid in parallel and made sure that the
   lower and upper bound on each rank was correct. This brings up a
   more general question: how to incorporate unit tests run from Lua
   using the main Lucee executable into the ctest system?

   The ``DataStruct.Field`` block allows both serial and parallel
   fields. Both need to be tested.
  
   I need to test the parallel Field from a unit test. This can be
   done by creating a field in parallel in which each local region is
   computed from a decomp while the same global region is used. This
   should create a field that behaves like a parallel field.

* January 12th 2012

  More reading up on Krommes 02. Made plans with Greg on how to move
  forward with the project. Will implement couple of schemes from
  Peterson & Hammett paper and then flux-reconstruction DG and Shu-DG
  for 2D incompressible flow problem.

* January 11th 2012

  Spent most of the day working on reviewing basic stuff on
  turbulence, reading Krommes's notes and other references. No work on
  Lucee. Eventually need to understand field-theory approach to
  deriving the GKE.

* January 10th 2012

  Creating a new org file for work done at PPPL. Completed a brief
  LDEVP on the parallel field implementation. Registered the
  decomposition objects so they can now be created from Lua. Next step
  is to hook these up the grid and field classes, implement sync() and
  test. Easier said than done.


  Now StructuredGridBase gets the decomposition object and uses it to
  compute the decomposition. Local and global regions are set
  correctly, at least in serial. Need to add tests for this.

  I am having some problems compiling the code in parallel: a bunch of
  undefined-symbol errors are showing up at link line. This probably
  due to a bad MPI build. I need to reactivate my Facetsall
  permissions and rebuild the complete tool chain. Grr ...

  FIXED parallel build problem. I am not sure if this is the correct
  way to do things. But builds for now. Next need to test the
  structured grid in parallel.

** TODO Create a new repo with regression tests.

   Just use TX's txtest system. It is good enough for our needs and
   will be one less thing to maintain.

** TODO Make neighbor calculations for periodic boundaries.

   A significant unresolved issue: how to deal with periodic domains?
   The neighbor calculation code needs to change for that. Essentially
   on each periodic side of the global region (including corners) we
   need to make copies of the global region. This will then give the
   proper neighbors, including self-intersections. Some ambiguity
   exists in the case in which the only one direction is
   periodic. Question: should the periodic conditions include corners
   in this case? I do not know, yet.
