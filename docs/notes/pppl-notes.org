# -*- org -*-

* January 31th - February 2nd

  Spent a significant amount of time building Lucee on
  portal.pppl.gov. This needed installation of new modules by the
  system admins as well as small tweeks to the code. Also, as usual,
  Lapack/Blas was an issue. For now I have gotten around it by using
  CLapack on portal.

  A rather nasty problem came up between CLapack and the fortran
  Lapack. This is the difference between a pointer to a single char
  (which is a char *) and a C string which is also char *. Turns out
  the Fortran version accepts both of these but the CLapack version
  only accepts the latter (i.e. NULL delimited string). As luck would
  have had it I was using the former. Switched to the latter to fix
  the problems.

  Spent a lot of time refereshing my memory with continous FEM. Turns
  out that the notation and formalism has been really screwed up by
  mathematicians. Now it is next to impossible to read these papers
  and texts without a thorough understanding of functional
  analysis. Apparently, FEM is porn to mathematicians. Better to DG
  before it gets this same brutal treatment.

* January 30th

  Need to extend Field class with multiple nodes. Need to take into
  account the possibility of using continous FEM which requires shared
  nodes between neighboring elements.

  Question: should we have a new data-structure, perhaps derived from
  Field or should Field itself be extended?

  One other option is: do not change Field at all. In fact, field
  should not know about "nodes" as nodes mean existence of a grid in
  which the nodes are located. Instead create a new FieldPtr type (or
  extend the existing one) to allow taking into account the nodes. The
  problem with this approach is that now somehow the FieldPtr needs to
  know about nodes. This could be done at construction time for the
  FieldPtr, for example, or set later on.

  One final option: do nothing. Let the user take care of this in the
  updater or functions that work on FEM type fields. This can be
  easily done by the user, but perhaps is not the best way to do it
  (but involves no work on my part). This is the approach I took in
  WarpX. Actually, this is the correct approach in the current
  framework. Introducing nodes does not make any sense as neither
  field or field-ptr can (or should) know about them.

* January 27th

  Working on MultiRegion class. This is taking longer than I expected,
  a classic symptom of a badly designed abstraction. Currently it is
  quite difficult to create the multi-region object due to the steps
  needed in the constructor. Need to simplify it. For example, one can
  imagine instead

#+BEGIN_EXAMPLE
  MultiRegion<2, int> multiRgn;

  int idx = multiRgn.addRegion( myRgn );

  // add more regions. At this point they are all unconnected

  // add connections (0 -> X, 1 -> Y)
  multiRgn.setRegionLowerConnection(idx, 0,
    MultiRegionConnectivity(targetIdx, targetDir, targetSide));

  // add more connections
#+END_EXAMPLE

  The advantage of this scheme is that unconnected sides do not need
  to be explictly added. The disadvantage is that creation phase might
  be longer and the user needs to keep track of the indices returned
  by the multi-region class. Of course, that could be eliminated by
  allowing the user to specify the index and then checking in the
  setRegionLowerConnection etc methods if such an index exists. In
  this case it would look like

#+BEGIN_EXAMPLE
  MultiRegion<2, int> multiRgn;
  multiRgn.addRegion( myIdx, myRgn );
#+END_EXAMPLE

** TODO Complete MultiRegion class

   Finish the iterator access (or get rid of it) and complete the
   code to allow adding connectivity information.
  
* January 19-24th

  Read first 3 chapters for Frisch.

  Added a new class MultiRegion that stores regions connected to each
  other. To avoid ambiguities in the connections the connectivities
  need to be specified in more detail than I initially thought. This
  is specially true when the block are connected to themselves in
  weird ways (branch-cut grids) or there is a direction switch
  involved at the seams.

  Partially read flux reconstruction paper by Huynh. A really good
  paper. The key difference between Huynh and Dumbser/Balsara approach
  is that the latter reconstruct a higher than K order polynomial
  using more information from the neighboring cells. Huynh only
  reconstructs enough to get K order continuous flux.

* January 18th 2012

  Fixed the sync() code and tested it. Seems to work. Will add more
  unit tests to make sure things are working correctly. Also noticed
  that the Field ctors were not seeting up global and local regions
  correctly. Fixed this. Now parallel simulations will be possible
  with Lucee! [Need to make sync() and decomp region to work with
  periodic BCs].

** TODO Add unit tests for getSendNeighbors() method

   I added the getSendNeighbors() method to compute the regions to
   which we should send data. This is not tested yet, although when
   used in the sync() method it seems to work just fine.

** Ctest for regression testing?

   Seems that ctest could be used for regression testing, at least for
   a simple stuff. Perhaps this should be investigated later but for
   now just use txtest as it has all the logic for finding queue on
   different machines.

** DONE Fix bug when send/recv neighbors are not the same

   Turns out that the case when send/recv neighbors are not the same
   has already bitten. When there are zero ghost cells on one (or
   more) edges of each sub-region the send and recv neighbors are
   different. The current getNeighbors() code only computes RECV
   neighbors (i.e. neighbors from which we expect to get
   something). Another call needs to be added for the SEND
   neighbors. This other call will compute neighbors by extending all
   other regions and intersecting with ourselves.

   I found this bug doing unit testing on the sync() code. Goes to
   show the importance of unit tests.

** Ownership of pointers

   In many classes pointers to externally created objects are
   stored. Should these be stored in boost shared pointers instead?
   What happens if the original pointer goes away. Also, in case of
   shared pointer is a consistent use of these needed?

* January 17th 2012

  Completed code to sync() structured fields. This does not work with
  periodic BCs yet.

  To test the sync() code I have had to add a siginificant amount of
  code in various grids and fields. This now allows creating a
  parallel field from C++ (rather than just Lua) and hence makes it
  easier to test.

  One question is: how can more than one region can be handled by a
  processor? This is a bit tricky as currenly the system implicitly
  assumes MPI will run one region on one processor. This needs to
  change.

* January 16th 2012

  Need to add other decomposition methods to allow arbitrary number of
  regions. Also, perhaps a pure Lua decomposition should also be
  allowed?

  If a field is created with `decompose=false` which processor should
  write the data? Currently all procs do this which can cause
  problems. One option is to not to "fix" this. From Lua one can do
  this by checking the rank and write the array if the rank is the
  correct one.

* January 13th 2012

  Extended the Field::writeToFile method to work in parallel. This was
  trickier than I thought as in some constructors the global region
  was not being set correctly. Fixed all this.

  Minor fixup: renamed globalBox -> globalRegion and localBox ->
  localRegion. This makes the code more consistent.

  Now that my facetsall access is enabled again I should be able to
  setup a regression test repo and see how it can be cron-ed at PPPL.

  Also, to allow unit testing I add methods Lucee.getRank() and
  Lucee.getNumProcs() to the top-level "Lucee" module so this
  information can be queried from Lua.

** DONE Add comprehensive unit test for parallel fields

   There are no unit tests for this stuff yet. However, I wrote a lua
   script to create a CartGrid in parallel and made sure that the
   lower and upper bound on each rank was correct. This brings up a
   more general question: how to incorporate unit tests run from Lua
   using the main Lucee executable into the ctest system?

   The ``DataStruct.Field`` block allows both serial and parallel
   fields. Both need to be tested.
  
   I need to test the parallel Field from a unit test. This can be
   done by creating a field in parallel in which each local region is
   computed from a decomp while the same global region is used. This
   should create a field that behaves like a parallel field.

* January 12th 2012

  More reading up on Krommes 02. Made plans with Greg on how to move
  forward with the project. Will implement couple of schemes from
  Peterson & Hammett paper and then flux-reconstruction DG and Shu-DG
  for 2D incompressible flow problem.

* January 11th 2012

  Spent most of the day working on reviewing basic stuff on
  turbulence, reading Krommes's notes and other references. No work on
  Lucee. Eventually need to understand field-theory approach to
  deriving the GKE.

* January 10th 2012

  Creating a new org file for work done at PPPL. Completed a brief
  LDEVP on the parallel field implementation. Registered the
  decomposition objects so they can now be created from Lua. Next step
  is to hook these up the grid and field classes, implement sync() and
  test. Easier said than done.


  Now StructuredGridBase gets the decomposition object and uses it to
  compute the decomposition. Local and global regions are set
  correctly, at least in serial. Need to add tests for this.

  I am having some problems compiling the code in parallel: a bunch of
  undefined-symbol errors are showing up at link line. This probably
  due to a bad MPI build. I need to reactivate my Facetsall
  permissions and rebuild the complete tool chain. Grr ...

  FIXED parallel build problem. I am not sure if this is the correct
  way to do things. But builds for now. Next need to test the
  structured grid in parallel.

** TODO Create a new repo with regression tests.

   Just use TX's txtest system. It is good enough for our needs and
   will be one less thing to maintain.

** TODO Make neighbor calculations for periodic boundaries.

   A significant unresolved issue: how to deal with periodic domains?
   The neighbor calculation code needs to change for that. Essentially
   on each periodic side of the global region (including corners) we
   need to make copies of the global region. This will then give the
   proper neighbors, including self-intersections. Some ambiguity
   exists in the case in which the only one direction is
   periodic. Question: should the periodic conditions include corners
   in this case? I do not know, yet.
