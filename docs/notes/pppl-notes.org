# -*- org -*-

#+TITLE:     Almost Daily Work Notes
#+AUTHOR:    Ammar Hakim
#+EMAIL:     ahakim@pppl.gov
#+LANGUAGE:  en

* April 15th

  Almost no work on the weekend.

  The mystery of energy conservation (or lack thereof) continues. I
  have carefully checked all code to make sure it is
  correct. Everything looks good. Now it seems to me that the real
  problem is that the gradient is computed with only first-order
  accuracy. This means that the gradient is not periodic, even though
  the solution is. In fact, the gradient even has opposite signs on
  the opposite boundaries! What this means is there is a term missing
  from the total energy conservation which is basically <phi
  n*grad.phi> integrated over the domain boundary. If grad.phi was
  exactly identical on the boundaries, this term would
  vanish. However, if the term was not the same it would contribute a
  non-zero term to the energy. In the solutions, the difference
  between the gradients on opposite sides is quite large.

  To take this into account I need to add the extra term to the energy
  updater. What a PITA.

* April 13 (Friday 13th)

** Scoping rules in Lua and Gkeyll modules

   Turns out that Lua needs explicit use of the "local" keyword for
   variables to be lexically scoped. This is very different than other
   languages in which variables are local to a scope by default. This
   has lead to some miserable problems in Gkeyll.

   The relative complexity of programming up a new Gkeyll simulations
   leads me to believe that there needs to be a module system. A
   developer would write a module, say for a particular problem, and
   provide a list of (simplified) input values that a user needs to
   specify in order to run the simulation. This means a user need not
   know all the gory details needed to run a simulation, but only
   remember a small set of, well documented, variables.

* April 12

  The following is a very bad way to test if a step failed:
  
#+BEGIN_EXAMPLE
  if (dtSuggested < myDt) then
#+END_EXAMPLE

  The reason is that the inequality can get terribly confused due to
  floating point percision errors. Instead should check the status
  flag. All my simulations have this problem, and so need to do a
  massive search and replace.

  For some reason the RK2 and RK3 results look identical. The
  differences are tiny. Why, I am not sure, but seems like RK3 behaves
  just like RK2.

* April 10-11

  Completed DynVector class and added some unit tests. Everything
  works. Now to use it through Lua, but first I need to add an updater
  to compute something useful.

  Completed an updater EnergyFromStreamFunctionUpdater that computes
  the net energy from the streamfunction. This work, or at least seems
  to. Put in total energy diagnostic into the 64x64 simulation. Turns
  out that with rk2() time-stepping the total energy *increases* by
  0.5%. The increase is not much, however, does indicate the mildly
  unstable nature of rk2() scheme. Need to implement rk3().

  Read Holloway paper. Basic point: using asymmetric Hermite
  polynomials for expanding the velcity dependence is better as it
  allows exact conservation of both momentum and energy, solves the
  plasma oscillation problem exactly and also preserves the shape of
  beams launched with specific velocities. As all non-dissapative
  discrete schemes it suffers from recurrence problem, i.e. phase
  mixing is simulated correctly only for a finite time after which the
  exponential decay turns into a an increase to give back the initial
  conditions.

** TODO Fix DataStruct::write() method to use sub-communicators

   The DynVector can not be written by all processors for obvious
   reasons. Hence, the DataStruct::write() method needs to be modified
   to allow a data-structure to take a sub-communicator so only a
   sub-set of processors do the I/O.

* April 9th

  Did a high-resolution simulation of the two-vortex problem. The
  results look good. The next step is to plot all the DOFs and not
  just the lower-left corner. For this I need to use the bi-linear
  representation to compute the solution on a finer mesh.

  I also need to figure out the problem with the double shear
  problem. Why is it "blowing up"? Is it really because of zero
  velocity at a node?

  Need to add the DynVector concept to Gkeyll.

* April 7th

  Studied the convergence of the 3rd and 4th order 1D Poisson
  solver. The schemes actually converge with 4th and 5th order
  accuracy. Perhaps this is an artifact of trying to measure
  asymptotic accuracy as even with 2 elements the solution looks
  rather good.

  Next need to study the 3rd order 2D Poisson solver. After that the
  periodic BCs solver with 2nd and 3rd order. All of this is very
  tedious work but essential to get confidence in the code.

  Setup a two-vortex problem. The solution looks really good even with
  128x128 grid points. Also setup a double shear problem. The solution
  does not look very good: apparently (I think) when the velocity
  switches sign the DG scheme does not work well. Need to investigate
  more as this is a problem with variable coefficient advection
  problems in general.

* April 6th

  Finally, have periodic BCs working with the FEM Poisson
  solver. Next, need to very carefully test it. Turns out that the
  problem was a very subtle one. The periodicity in FEM means that the
  periodic nodes needs to be identified carefully otherwise all hell
  breaks loose. This was probably the worse week of debugging, both
  the math and the code, in a long time. Now I can sleep.

* April 5th

  Why aren't the far away nodes appearing in the stiffness matrix for
  periodic BCs?

* April 4th

  Need to now implement a generic diagnostics mechanism. The first
  step is to add a new DataStruct called (perhaps) DynVec. This is the
  name I used in Facets and is good enough here. An example to store
  the total energy would be

#+BEGIN_EXAMPLE
  energy = DataStruct.DynVec { numComponents = 1 }
#+END_EXAMPLE

  which would create space to store the total energy in the
  system. The actual computation of the energy would take place in a
  special updater. One can imagine doing similar stuff as done for the
  BCs:

#+BEGIN_EXAMPLE
  energyDiag = Diagnostic.Energy {}
  enstrophyDiag = Diagnostic.Enstrophy {}

  diag = Updater.Diagnostics2D {
    onGrid = grid,
    diagnostics = {energyDiag, enstrophyDiag},
  }
  diag:setIn( {field} )
#+END_EXAMPLE

  Worked more on the periodic BCs issue. I think the basic idea is now
  correct and implemented. However, the solution is still
  incorrect. This could be because I am not taking into account the
  effect of the top-right node on the bottom left node. It also looks
  like Dirichlet BCs are being effectively applied. Tomorrow I need to
  print the code out and pore over it very, very carefully.

* April 3rd

  Perhaps I have now figured out the problem with my Poisson solver
  with periodic BCs. The issue is that although the right (and top)
  edges are set correctly, the periodicity on the left edge is not
  taken into account correctly. This causes the system to be
  ill-posed, I think. To fix the effect of the next to last cells on
  the top and right edges will need to be taken into account when
  constructing the stiffness matrix and the sourcet terms, specially
  for the cells on the left and bottom edges. Not done this yet, but
  need to.

  To get out of this periodic BC debugging madness, I setup and ran a
  simulation with two vortices in a box. The results look fine which
  makes me more confident that the basic Poisson bracket and Poisson
  solver algorithms are working correctly.

* April 2nd

  Spent all day trying to find bug in periodic BCs. No good. I now
  suspect that the formulation of the problem in periodic BCs itself
  might be incorrect. For example: for periodic BCs not only the
  solution but also the slope should match. However, this does not
  seem to be happening in the computed solutions, although the
  solution is periodic. Will spend some more time tomorrow otherwise
  will move to implementing a small stand-alone solver to test things.

* March 30

  After much investigation I have realized that the periodic BC code
  is not correct. It seems to work in some situation which led me to
  believe it was working. However, for the double shear problem the
  solution looks completely bogus and very simple tests now show a
  problem in 2D with just 2 cells in the Y-direction (even though
  there is no variation in Y). Spent time debugging but to no avail.

* March 29th

  False start on getting Poisson solver to work with periodic
  BCs. Half the day was wasted till I realized what was going on.

  Modified Poisson solver to work with periodic BCs. For some crazy
  reason the solution looks as if one is applying Dirichlet BCs and
  not periodic BCs. Not sure what is going on, but more staring at the
  code is needed.

  FOUND THE BUG: The problem was that Dirichlet BCs were being applied
  even when periodic BCs were specified. This is just bad programming
  and wasted another 1/2 day. So day is now over.

  Strangely, the KSP solver has no problem converging to a solution
  even when BCs are periodic. Not sure why, as the matrix should not
  posses an inverse in this case. NOTE: This actually does not work in
  general. So had to pin the lower-left corner value to get
  convergence.

  Setup a double shear problem. This is not working and there seems to
  be some problem with the boundary condition.

* March 28th

  Now polyOrder 2 also works. In getting this to work the code had to
  be rearranged a bit, but now will work with any basis
  functions. This generalization includes a loop over direction which
  seems to add a 10% overhead. For some reason the compiler is unable
  to unroll the loops even though the loop size is explicitly set.

  One lesson here is that even small things can have an impact on the
  performace and that the code performs no where close to its optimal
  levels. This is okay for now but later when real physics problems
  are being tackled it might be important to carefully optimize the
  code.

  Added a flag to the Poisson solver to allow a DG field as an
  input. Now we are really ready for the coupled problem.

  Fixed a very nasty but subtle bug in the Poisson solver that was
  giving weird results when the Poisson solver was called multiple
  times. Turns out that the RHS of the poisson equation was not being
  cleared properly before setting it in a time-dependent problem,
  causing the solution to be different even if the source did not
  change between calls.

* March 27th

  Did more basis tests of the Poisson bracket updater. Converted it to
  be more systematic and eventually be used as a proto-type for a
  dimensionally independent DG solver for other hyperbolic systems.

  Tried to compute the matrix-vector multiplies using BLAS. Makes the
  code 5X *slower*. I suspect this is because BLAS has no advantage
  over simple loops when the matrices and vectors are small. Perhaps
  it would make more sense when the complete updater is
  "vectorized". However, it seems there is a lot of room for
  improvement in performance here.

  Added the methods to support polyOrder = 2. However, the Poisson
  updater still needs more work to make it independent of the number
  of nodes on the faces. Will do this tomorrow, getting very tired
  now.

* March 26th

  Completed the surface integral terms needed in the Poisson bracket
  updater. This involve some more work to the basis function classes,
  making them even chubbier. The interface is becoming very large and
  cumbersome and needs to be looked at again, eventually.

  The Poisson bracket updater is not crashing but also does not seem
  to produce the correct results. Need to debug.

  Found bug in the Poisson bracket updater! It was not actually a bug,
  but I had not implemented upwinding which made the solution show
  oscillations on the trailing edge. Once upwinding was implemented
  the algorithm seems to work fine.

  For now I am testing on a problem with only variations in
  X-direction. Next need to clean up the updater and then do more
  careful tests, including in 2D.

** TODO Write up notes on nodal basis functions

   The interface is sufficiently complicated that an explanation is
   required on how to compute the various things needed in the solvers
   (CG and DG) for a new set of basis. Also, the document should
   explain the CG/DG algorithms in context of the inviscid Euler/H-W
   work we are doing now.

** TODO Put gkeyll docs on ammar-hakim.org/gkeyll

   Put the docs and tech-notes for easy reference. We are close to a
   first-application perhaps in drift-wave turbulence as described by
   the Hasegawa-Watakani equations.

* March 22nd

  Completed all basic loops for Poisson bracket operator. Final step
  is to hook in the surface integral terms. For this a "face mass
  matrix" needs to be computed.

  Wrote an input file with constant prescribed streamfunction with
  evolving vorticity. Will use as a test case to test just the Poisson
  bracket operator.

** DONE Fix crash on using duplicate()-ed fields in out

   Turns out that the code is crashing when using fields created using
   the duplicate() Lua method. Need to investigate and fix.

   PROBLEM: The rgnIdx field in the duplicated field is not
   correct. This is probably the cause of the crash. Will fix in the
   morning. Too tired tonight. NEED TO ADD UNIT TEST FOR DUPLICATE
   METHOD TO ENSURE THIS PROBLEM IS CHECKED FOR.

* March 21st

  Computed all matrices needed in the nodal DG solve. Next to hook
  these into the main loop to compute the various terms.

  Spent some time reading about Hasegawa-Wakatani model. Turns out
  this will need more than just a Poisson solve and a Poisson bracket
  operator: extra terms appear which need to be computed. However,
  they are not hard to do and involve just some more application of
  the differentiation matrices. Derivation in Balescu is very
  enlightening as he uses too many symbols making the derivation very
  un-transparent.

  Compiled code on portal. Petsc fails to build, so no Poisson
  solver. Need to spend time on why this is the case. Perhaps on the
  weekend, after the Poisson bracket operator is complete.

** DONE Test EvalOnNodesUpdater on polyOrder = 1 and 2 in 2D

  Need to test this so we know there are no issues with this. For each
  polyOrder there needs to be two tests: one for a CG field and the
  other for DG field.

* March 20th

  Added new method getGradStiffnessMatrix to NodalFiniteElementIfc
  class. This will support the creation of the final needed matrix in
  the nodal DG scheme. The SerendipityElement2D setup functions are
  now looking horrendous. However, eventually I will need to replace
  all of these with numerically (rather than analytically) computed
  basis functions as the current implementation will not work on
  general quadrilateral geometries. At that point need to copy the
  current implementation into the proto directory and rename it so it
  is available to test the new numerical basis.

* March 19th

  Again, no work on weekend. Or almost none. On Sunday night I played
  around with comparing DG with WAVE for 1D Maxwell equations. The
  point here was that in the IBW problem which I did for David S
  before leaving TX one observes that the RF wave decays very rapidly
  as it propagates into the plasma. This is a big issue, and so I
  investigated a 4th order DG scheme for this. Turns out that the 4th
  order DG is much better: in fact, the decay is eliminated and it
  also runs faster! Unfortunately, this will not impact David's
  project at present, but still sent him my findings. Need to write up
  3 pages with my cold fluid results for him. Need to do this soon as
  the Phase II due date is 4/4.

  Found a bug that has been driving me nuts for a while: the modal 1D
  DG was not working when the time-step was being adapted,
  i.e. rejected and retaken. Turns out that after staring at the C++
  code for a very long time (all day today) I realized that the
  problem was really with the Lua script! Basically, when a step is
  retaken somehow the previous state of the solution is lost. This is
  an elementary mistake which has cost me a lot misery. Lesson: do not
  make elementary mistakes.

* March 16th

  Completed methods needed to have 1D Lobatto elements work with
  polyOrder > 1. This proved to be easy but turned into a debugging
  headache due to a "trivial" bug I introduced.

  The methods for 2D Serendipity elements with polyOrder > 2 are
  considerably tricker to implement. Turns out that the missing nodes
  in the interior make the local -> global mapping really
  hairy. However, I think this is now correct.

  On the other hand the methods copyAllDataFromField and
  copyAllDataToField are turning out to be challenging. The mapping
  from a field to the flat array needed in PetSc is very confusing and
  I need to figure this out before the Serendipity elements can be
  used for polyOrder 2.

  Finally figured on how to do the copyAllDataToField and
  copyAllDataToField methods correctly. The basic idea is to provide a
  new method getGlobalIndices() which given the current cell index
  return the list of owned global indices in that cell and the
  corresponding local node numbers. With this the methods are simple
  to implement.

  The Poisson solver now seems to work for polyOrder = 2 (eyeball
  metric). Not yet fully verified, and that is the next thing to do.

* March 15th

  Wrote EvalOnNodesUpdater that initializes a nodal field from a Lua
  function. Seems to work, although more careful testing is
  needed. 

  One major issue is how to plot these fields? VizScheme/Visit does
  not work for such grids, at least now, although plans are afoot at
  TX to extend Visit and VizSchema to support such meshes. An option
  for now is to write a Python script that converts the H5 output to
  VTK which is then used for plotting. Conversion is not such a big
  deal for 2D fields but can become very painful (slow) in 3D.

  Turns out that the polyOrder 2 Serendipity elements are nasty: it is
  not trivial to figure out the number of global nodes or the local ->
  global mapping. My initial simplistic approach is completely wrong,
  which I discovered on trying to solve the Poisson equation using
  these more complicated elements.

  Formulated the nodal DG scheme. At this point I understand how every
  term works. Next to implement this for the Poisson bracket operator.

** A genuine nodal field type

   The nodal CG/DG schemes need a special nodal field type. This
   method of storing the nodal data in extra components of the Field
   class is not a maintainable model for the long run. So how to do
   this? One option is to introduce a field type that depends on the
   NodalFiniteElementIfc derived classes. Essentially, the first step
   would be to define a grid, then a element basis and then create
   fields which take these element basis as input. For example:

#+BEGIN_EXAMPLE

   -- create grid
   grid = Grid.RectCart2D { ... }

   -- create basis
   lobattoBasis = NodalFiniteElement2D.Serendipity {
     onGrid = grid,
   }

   -- now construct fields
   phi = DataStruct.NodalField {
     onBasis = basis,
     numComponents = 5,
     shareCommonNodes = true, -- true for CG, false for DG
     ...
   }
#+END_EXAMPLE

   The nodal fields object now allocates enough space depending on
   "shareCommonNodes" flag which tells it if overlapping nodes between
   cells are shared or not.

* March 14th

  Added more regression tests. Added an optional message to
  UpdaterStatus class that allows updater to tell Lua what happened,
  specially in case of failure.

* March 13th

  Added more regression tests. Even more are needed as the code is now
  undergoing major changes and additions and it is important not to
  regress on existing functionality.

  Converted the Poisson FEM solver into a dimension independent and
  nodal basis function agnostic updater. Tested by comparing with
  exact solution and also did a convergence study. Wrote up a
  Simulation Journal entry on this. The higher than second order basis
  have not been benchmarked yet. This awaits completion of the
  initialization updater for nodal FEM fields.

  The construction of the Petsc matrix (stiffMatrix) is taking a very
  long time. For example, in 2D 64x64 grid the setup takes 50x more
  than the inversion.

  This is a problem I have dealt with before: the solution is to
  pre-allocate the matrix with as much information as possible (number
  of non-zero entries per row is critical, for example). This makes
  the setup much faster. Also, the solve itself is rather
  inefficient. Of course, the defaults are used as-is and so there is
  a lot of room for improvement.

** TODO Fix parallel test

  Turns out that the sodshock test hangs in parallel. Of course this
  needs to be debugged ASAP. This is probably a run-away send/recv
  which might be very nasty to debug. Grr ....

  Perhaps later tonight as I need to focus on the Poisson bracket
  algorithm for now.

*** Note added on 3/19

    Is this a manifestation of the getSendNeighbors and
    getRecvNeighbors?

* March 12th

  No work on weekend.

  Now have the 2D Poisson solver working. This is basis-function
  agnostic as well as dimension agnostic. So should merge the 1D and
  2D updaters into one and test each of these carefully.

  Also, renamed the top-level executable to gkeyll to reflect that
  this software will be used for solution of GKE.

  Added even more functions to the NodalFiniteElementIfc
  class. Several more will be still needed for the DG scheme.

** DONE Make parallel input files work in serial

   There should not be two different input files for serial and
   parallel. The same file should work with both. Also, ensure that
   the write method works with ghost cell-write in parallel.

   One place this can be taken care off is the StructuredGridBase
   class when the decomposition is created: basically, in serial the
   decomposition should not be sought out at all.

* March 9th

  Made the Poisson solver completely agnostic of the basis functions
  used. Now once I figure out how to apply the BCs in 2D the solver
  can be rewritten to be dimensionally indenpendent as well as work on
  a mapped grid.

  Added BCs to 2D Poisson FEM updater. Does not completly work yet.

  Make all Lucee::LuaTable methods const-correct. I do not remember
  why this was not done in the first place. Perhaps laziness or just
  negligence?

** DONE Extend UpdaterStatus to take a message on why step failed

   This will allow some semblance of debugging, specially with
   linear and non-linear system solves with FEM.

* March 8th

  Compared 1D FEM Poisson solver with exact solution. Found that I had
  a sign off. Fixed and now FEM solution compare well with exact
  solution. Still need to do convergence study, etc.

  Also, I need to implement a method to initialize nodal FE/DG
  fields. The two-node Lobatto elements work fine now as the fields
  allow "nodal" storage. Of course, this will not work when there are
  interior nodes to be initialized.

  Working on a 2D Poisson FEM updater. This seems is very similar to
  the 1D updater and perhaps a dimension-independent updater can be
  written. Best would be if it worked also on a mapped grid.

  Somehow need the basis function IFC class to return the manner in
  which the nodes are laid out. Otherwise I do not see how data can be
  extracted from a field in a transparent manner in the Poisson (or
  other) FE updaters. I.e: the updater should really be agnostic of
  the node layout, the number of nodes, etc.

** TODO Convert Poisson solver and related classes (NodalFiniteElementIfc) for parallel

   The Poisson solver and related classes only work in serial. Need to
   convert these to parallel. Perhaps this is not hard, but will need
   to look into the complete chain of classes, including

** TODO Add more regression tests
   
   Need to add many more regression tests to the system.

* March 7th

  Have a working 1D FEM Poisson solver. Needs testing to make sure the
  solutions are correct. I printed out the stiffness matrix and the
  RHS to ensure that they are correct. So KSP inversion should be
  correct, one would hope.

  Extended the field I/O method to allow writing ghost (or part of
  ghost) cells. This ensures the nodes on the right-most (top-most,
  ...) get written to Hdf5 and allow correct viz of FEM type solution.

  This is actually not the correct way as the interior nodes in the
  ghost cells will also be written out. This is not the desired
  behaviour as only the edge nodes are needed. The correct solution
  here is to actually have a nodal FE field that properly takes into
  account shared nodes between cells. This is a much more complex task
  than I am ready to tackle at present and needs to be done in the
  future. See March 6th note on "Flat field" below.

** TODO Should one add a "finalize" method to UpdaterIfc?

   This will allow "unloading" an updater data from Lua script if
   needed. Not really critical for now, but something to keep in mind.

* March 6th

  Working on 1D Poisson solver using FEM method. Setup basic class,
  brought in Petsc and tested that stiffness matrices are correctly
  built. Found a bug: one can not use the '=' operator for copying
  matrix values as this creates a shallow copy of the RHS. Instead,
  the copy() method needs to be used.

  The 2D Poisson solver should not be that much different, which I
  will work on next. Once that is completed I will switch to DG, which
  should be much simpler. Even though DG needs more complicated
  information (Riemann solves, limiters, ...) it is actually an easier
  scheme to implement.

** Flat field for FEM/DG scheme

   It might be valuable to introduce a "flat-field" data structure
   that stores data essentially in a linear array. This field would be
   indexed with two indices (always): cell-index and
   node-index. Additionally, number of components would be
   specifiable. [This basically is just Field2D.]

   The looping into this field would be achieved by specialized
   iterators, that also would allow neighbor calculations.

* March 2nd

  Implemented Euler numericalFlux method. Did a basic test with
  Sod-shock. The results look "almost" correct, however, there is an
  error: the shocks do not move at the correct speed and the results
  do not compare with either exact solution or with miniwarpx
  results. The problem is most likely in the DG updater for multiple
  equations, which I need to find and fix. 

  [The error could be in the normalization coefficients when applied
  to the case of more than one equation].

  The great thing about the DG method is that only the numerical flux
  method is needed, at least for case in which limiters are not
  applied: the complete decomposition is only needed for the limiters.

* February 21-March 1st
  
  Working on FEM Poisson solver. To get this correct I need to define
  nodal basis functions. This has taken longer than I expected because
  I want to implement this in a way that the basis functions can be
  used in DG also.

  Hooked in PetSc build into luceeall. The parallel build fails but
  the serial code is good enough for now.

  Spent a lot of time working out the various serendipity and
  cartesian product nodal basis functions. Finally have all of these
  figured out, at least for rectangular grids. For now this is okay
  for testing, but eventually will need to be extended to general
  quadrilateral cells. In that case the matrices will need to be
  computed numerically (rather than analytically).

  Implemented 1d Lobatto basis function upto polynomial order 3 (4
  node elements). I did this to get write a 1D Poisson solver to get
  some experience with FEM.

* February 20th

  Formulated FEM scheme for 1D Poisson equations. The key step is the
  one that goes from the local stiffness matrix to the global
  stiffness matrix via the connectivity matrix. For 1D Poisson
  equation the resulting discrete system look like a second-order
  central difference approximation for the spatial operator with an
  averge for the source that weights the current note by 2/6 and
  neighbors by 1/3. Next need to write out the 2D version of this.

* February 18th

  Created a repo to house regression tests. Tried to use txtest but
  was too complicated to use, at least for now. Switched to WarpX
  regression system. Needs more work but works fine for now.

* February 17th

  Turns out that the detector based on Krivodonova et. al. is not so
  good. It is not invariant to addition of a constant to the solution
  for advection equation, for example. Need to think of this more
  carefully.

* February 16th

  Explored a possible detector for discontinuities for use in DG
  scheme. This seems to work okay, but needs some more
  exploration. When applied to the DG scheme itself it does not
  improve the solution a whole lot. The problem is that the detector
  kicks in (I think correctly) even in smooth regions as the slopes
  get modified by the DG update. It seems that a good limiter is also
  needed besides a good detector. Otherwise one may save on compute
  time but improve accuracy.

** DONE Apply limiters to initial conditions

  It also occurs to me that the initial condition needs to be limited
  in the DG Lua code.

* February 14th

  Need to look carefully at both wave and DG schemes. The efficiency
  can be probably improved significantly, at least by a few factors if
  not an order of magnitude.

  The DG limiter is terrible. It completely wipes out the smooth
  exterma. Need to develop something better. The Suresh and Huynh
  paper is really dense and hard to understand. Very unlike his flux
  reconstruction paper which is clear and easy to understand.

* February 13th

  Added new methods to the HyperEquation class to project a vector on
  left-eigenvectors and reconstruct them with
  right-eigenvectors. These two operators are inverses of each
  other. I.e. first projecting on left-eigenvectors and then
  reconstructing on right-eigenvectors should give the original vector
  back. These methods were added for use in limiters for DG scheme.

  The HyperEquation class is becoming very "fat". However, this is
  okay as not all methods are required for all schemes. These two new
  methods will also allow (in combination with the numericalFlux
  method) the implementation of the MUSCL scheme in Lucee.

  Added an updater to limit solution and/or projection from
  DG. Implemented characteristic limiter. Should also implement
  componentwise limiter and then updater Euler equation class with the
  methods needed to make it work with DG.

* February 12th

  I changed the modal DG to return a first order forward Euler update
  and not the "tendencies". I am not sure if this is the correct thing
  to do and perhaps it is a mistake. However, this does allow easy
  application of limiters after the first-order update is complete.

* February 10th

  The algorithm now works! The problem was not with the C++ code but
  with the Lua program. Turns out that the accumulate function is
  actually quite confusing to use as the current contents of the
  fields are not reset before accumulation. Of course, this is the
  correct and intended behavior. Perhaps the solution is to introduce
  a new method called "combine" that clears the current content of the
  field and then does the accumulation. This would be like assigning a
  field with a linear combination of other fields. Spent too much time
  on debugging this.

  Compared with miniwarpx solutions. The timing of miniwarpx v/s
  optimized lucee are comparable. However, I am not sure if miniwarpx
  was built with full optimization. I need to check in the code
  somewhere and build it to do a fair comparison.

  The DG efficiency could be improved by careful rearrangement of the
  loops to make sure the updates happen in cache-correct
  sequence. Anyway this is not too critical at this stage.

** DONE Add a 'combine' method to Field

   This will combine a set of fields into a single one. Essentially it
   a call to clear() followed by an accumulate.

** DONE Put miniwarpx into a bitbuket repo

   This is a good code that allows easy comparison for testing. Should
   check it into bitbuket and make sure it can be built. Perhaps even
   CMake it.

* February 9th

  Working on 1D modal DG. This updater returns the increment in the
  solution. Hence, using its output one can easily do any RK
  time-stepping in the Lua code.

  Completed the code for the 1D modal DG method. The algorithm seems
  to be basically working but the solution is slowly increasing. Need
  to investigate why, perhaps there is an error in the normalization.

** DONE Extend 'accumulate' method

  Need to extend the luaAccumulate method to take in arbitrary number
  of fields and coefficients. For example
#+BEGIN_EXAMPLE
  qNew:accumulate(1.0, q, 0.5, dq)
#+END_EXAMPLE
  will set qNew = qNew + q + 0.5*dq.

* February 7th

  Completed ProjectOnBasisUpdater to compute projection of a Lua
  function on Legendre polynomials. The coefficients are stored in
  row-major order.

** DONE Add initialize() to BasicObj class

   Add this method and call it immediately after readInput() method in
   the ObjRegistry::makeLuaObj method (Line 91). This will eliminate
   the need to explicitly call this method.

   With this change *every* Lua script will need to be changed to
   remove the explicit call to initialize().

** TODO Why are in/out not present in the UpdaterIfc table? Fix if needed.

   There perhaps was some reason for this which I no longer
   recall. But it would make life easier if this was a part of the
   Updater table and did not need an explicit step to do.

* February 6th

  Added an interface class for quadrature weights and
  ordinates. Implemented specific case of Gaussian quadrature.

  Need a way to project a function on basis function for use DG. To do
  this the quadrature object should be created and then used to
  initialize a field whose components represent the coefficients of
  expansion.
#+BEGIN_EXAMPLE
  quad = QuadratureRule.Gaussian { numNodes = 2 }

  -- let q be a field and initFunc a Lua function
  q:project(initFunc, plOrder, quad)
#+END_EXAMPLE

  This will intialize the components of q to the projection of
  initFunc on Legendre polynomials of order plOrder. Perhaps in the
  future projection on different basis could also be suppoeted. Note
  that by using the alias method one can currently set the average (or
  projection on P_0) rather easily. However, this will lead to less
  accurate solutions as the higher order coefficients will not be set.

** Bizarre behavior of luaL_ref method

   Seems like luaL_ref pops the stack and leaves it in a very unstable
   situation. This means that after this method is used it is possible
   that the remaining functional parameters might be totally messed
   up. So, luaL_ref should be done *last*.

   One of the lessons here is that I need to start testing the Lua
   scripts so all Lua callable methods are exercised. I am loosing
   confidence in the code due to lack of regression tests. Time to
   pull in txtests.

** A wasted day: project method will not work

   I am unable to figure out a clean way to make the project method
   work. In fact, I now think that it might be too much of a headache
   to do so as the method is becoming horribly complex.

   It is better to write an updater that does this instead. Will do
   that tomorrow. A big waste. A possible solution is to create an
   updater like the following.

#+BEGIN_EXAMPLE
  initField = Updater.ProjectOnBasis1D {
    onGrid = grid,
    numBasis = 2,
    project = function (x,y,z,t)
                -- do something here
              end,
  }
  initField:initialize()
  initField:setOut( {q} )

  -- run initialization updater
  initField:advance( 0.0 )
#+END_EXAMPLE

* January 31th - February 2nd

  Spent a significant amount of time building Lucee on
  portal.pppl.gov. This needed installation of new modules by the
  system admins as well as small tweeks to the code. Also, as usual,
  Lapack/Blas was an issue. For now I have gotten around it by using
  CLapack on portal.

  A rather nasty problem came up between CLapack and the fortran
  Lapack. This is the difference between a pointer to a single char
  (which is a char *) and a C string which is also char *. Turns out
  the Fortran version accepts both of these but the CLapack version
  only accepts the latter (i.e. NULL delimited string). As luck would
  have had it I was using the former. Switched to the latter to fix
  the problems.

  Spent a lot of time refereshing my memory with continous FEM. Turns
  out that the notation and formalism has been really screwed up by
  mathematicians. Now it is next to impossible to read these papers
  and texts without a thorough understanding of functional analysis.

* January 30th

  Need to extend Field class with multiple nodes. Need to take into
  account the possibility of using continous FEM which requires shared
  nodes between neighboring elements.

  Question: should we have a new data-structure, perhaps derived from
  Field or should Field itself be extended?

  One other option is: do not change Field at all. In fact, field
  should not know about "nodes" as nodes mean existence of a grid in
  which the nodes are located. Instead create a new FieldPtr type (or
  extend the existing one) to allow taking into account the nodes. The
  problem with this approach is that now somehow the FieldPtr needs to
  know about nodes. This could be done at construction time for the
  FieldPtr, for example, or set later on.

  One final option: do nothing. Let the user take care of this in the
  updater or functions that work on FEM type fields. This can be
  easily done by the user, but perhaps is not the best way to do it
  (but involves no work on my part). This is the approach I took in
  WarpX. Actually, this is the correct approach in the current
  framework. Introducing nodes does not make any sense as neither
  field or field-ptr can (or should) know about them.

* January 27th

  Working on MultiRegion class. This is taking longer than I expected,
  a classic symptom of a badly designed abstraction. Currently it is
  quite difficult to create the multi-region object due to the steps
  needed in the constructor. Need to simplify it. For example, one can
  imagine instead

#+BEGIN_EXAMPLE
  MultiRegion<2, int> multiRgn;

  int idx = multiRgn.addRegion( myRgn );

  // add more regions. At this point they are all unconnected

  // add connections (0 -> X, 1 -> Y)
  multiRgn.setRegionLowerConnection(idx, 0,
    MultiRegionConnectivity(targetIdx, targetDir, targetSide));

  // add more connections
#+END_EXAMPLE

  The advantage of this scheme is that unconnected sides do not need
  to be explictly added. The disadvantage is that creation phase might
  be longer and the user needs to keep track of the indices returned
  by the multi-region class. Of course, that could be eliminated by
  allowing the user to specify the index and then checking in the
  setRegionLowerConnection etc methods if such an index exists. In
  this case it would look like

#+BEGIN_EXAMPLE
  MultiRegion<2, int> multiRgn;
  multiRgn.addRegion( myIdx, myRgn );
#+END_EXAMPLE

** TODO Complete MultiRegion class

   Finish the iterator access (or get rid of it) and complete the
   code to allow adding connectivity information.
  
* January 19-24th

  Read first 3 chapters for Frisch.

  Added a new class MultiRegion that stores regions connected to each
  other. To avoid ambiguities in the connections the connectivities
  need to be specified in more detail than I initially thought. This
  is specially true when the block are connected to themselves in
  weird ways (branch-cut grids) or there is a direction switch
  involved at the seams.

  Partially read flux reconstruction paper by Huynh. A really good
  paper. The key difference between Huynh and Dumbser/Balsara approach
  is that the latter reconstruct a higher than K order polynomial
  using more information from the neighboring cells. Huynh only
  reconstructs enough to get K order continuous flux.

* January 18th 2012

  Fixed the sync() code and tested it. Seems to work. Will add more
  unit tests to make sure things are working correctly. Also noticed
  that the Field ctors were not seeting up global and local regions
  correctly. Fixed this. Now parallel simulations will be possible
  with Lucee! [Need to make sync() and decomp region to work with
  periodic BCs].

** TODO Add unit tests for getSendNeighbors() method

   I added the getSendNeighbors() method to compute the regions to
   which we should send data. This is not tested yet, although when
   used in the sync() method it seems to work just fine.

** Ctest for regression testing?

   Seems that ctest could be used for regression testing, at least for
   a simple stuff. Perhaps this should be investigated later but for
   now just use txtest as it has all the logic for finding queue on
   different machines.

** DONE Fix bug when send/recv neighbors are not the same

   Turns out that the case when send/recv neighbors are not the same
   has already bitten. When there are zero ghost cells on one (or
   more) edges of each sub-region the send and recv neighbors are
   different. The current getNeighbors() code only computes RECV
   neighbors (i.e. neighbors from which we expect to get
   something). Another call needs to be added for the SEND
   neighbors. This other call will compute neighbors by extending all
   other regions and intersecting with ourselves.

   I found this bug doing unit testing on the sync() code. Goes to
   show the importance of unit tests.

** Ownership of pointers

   In many classes pointers to externally created objects are
   stored. Should these be stored in boost shared pointers instead?
   What happens if the original pointer goes away. Also, in case of
   shared pointer is a consistent use of these needed?

* January 17th 2012

  Completed code to sync() structured fields. This does not work with
  periodic BCs yet.

  To test the sync() code I have had to add a siginificant amount of
  code in various grids and fields. This now allows creating a
  parallel field from C++ (rather than just Lua) and hence makes it
  easier to test.

  One question is: how can more than one region can be handled by a
  processor? This is a bit tricky as currenly the system implicitly
  assumes MPI will run one region on one processor. This needs to
  change.

* January 16th 2012

  Need to add other decomposition methods to allow arbitrary number of
  regions. Also, perhaps a pure Lua decomposition should also be
  allowed?

  If a field is created with `decompose=false` which processor should
  write the data? Currently all procs do this which can cause
  problems. One option is to not to "fix" this. From Lua one can do
  this by checking the rank and write the array if the rank is the
  correct one.

* January 13th 2012

  Extended the Field::writeToFile method to work in parallel. This was
  trickier than I thought as in some constructors the global region
  was not being set correctly. Fixed all this.

  Minor fixup: renamed globalBox -> globalRegion and localBox ->
  localRegion. This makes the code more consistent.

  Now that my facetsall access is enabled again I should be able to
  setup a regression test repo and see how it can be cron-ed at PPPL.

  Also, to allow unit testing I add methods Lucee.getRank() and
  Lucee.getNumProcs() to the top-level "Lucee" module so this
  information can be queried from Lua.

** DONE Add comprehensive unit test for parallel fields

   There are no unit tests for this stuff yet. However, I wrote a lua
   script to create a CartGrid in parallel and made sure that the
   lower and upper bound on each rank was correct. This brings up a
   more general question: how to incorporate unit tests run from Lua
   using the main Lucee executable into the ctest system?

   The ``DataStruct.Field`` block allows both serial and parallel
   fields. Both need to be tested.
  
   I need to test the parallel Field from a unit test. This can be
   done by creating a field in parallel in which each local region is
   computed from a decomp while the same global region is used. This
   should create a field that behaves like a parallel field.

* January 12th 2012

  More reading up on Krommes 02. Made plans with Greg on how to move
  forward with the project. Will implement couple of schemes from
  Peterson & Hammett paper and then flux-reconstruction DG and Shu-DG
  for 2D incompressible flow problem.

* January 11th 2012

  Spent most of the day working on reviewing basic stuff on
  turbulence, reading Krommes's notes and other references. No work on
  Lucee. Eventually need to understand field-theory approach to
  deriving the GKE.

* January 10th 2012

  Creating a new org file for work done at PPPL. Completed a brief
  LDEVP on the parallel field implementation. Registered the
  decomposition objects so they can now be created from Lua. Next step
  is to hook these up the grid and field classes, implement sync() and
  test. Easier said than done.


  Now StructuredGridBase gets the decomposition object and uses it to
  compute the decomposition. Local and global regions are set
  correctly, at least in serial. Need to add tests for this.

  I am having some problems compiling the code in parallel: a bunch of
  undefined-symbol errors are showing up at link line. This probably
  due to a bad MPI build. I need to reactivate my Facetsall
  permissions and rebuild the complete tool chain. Grr ...

  FIXED parallel build problem. I am not sure if this is the correct
  way to do things. But builds for now. Next need to test the
  structured grid in parallel.

** DONE Create a new repo with regression tests.

   Just use TX's txtest system. It is good enough for our needs and
   will be one less thing to maintain.

** TODO Make neighbor calculations for periodic boundaries.

   A significant unresolved issue: how to deal with periodic domains?
   The neighbor calculation code needs to change for that. Essentially
   on each periodic side of the global region (including corners) we
   need to make copies of the global region. This will then give the
   proper neighbors, including self-intersections. Some ambiguity
   exists in the case in which the only one direction is
   periodic. Question: should the periodic conditions include corners
   in this case? I do not know, yet.
